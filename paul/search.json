[
  {
    "objectID": "feed.html",
    "href": "feed.html",
    "title": "Slides",
    "section": "",
    "text": "Difference in Differences II\n\n\nECON526\n\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\n  \n\n\n\n\nECON 526: Quantitative Economics with Data Science Applications\n\n\n\n\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\n  \n\n\n\n\nFixed Effects\n\n\nECON526\n\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Difference in Differences\n\n\nECON526\n\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\n  \n\n\n\n\nMatching\n\n\nECON526\n\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\n  \n\n\n\n\nSynthetic Control\n\n\nECON526\n\n\n\n\n\n\n\n\n\nPaul Schrimpf\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ECON 526: Quantitative Economics with Data Science Applications",
    "section": "",
    "text": "Slides\n\nMatching slides, notebook\n\nReading: chapters 10-12 of Facure (2022) and chapter 14 of Huntington-Klein (2021)\n\nIntroduction to difference in differences, notebook\n\nReading: chapter 13 of Facure (2022)\n\nFixed Effects, notebook\n\nReading: chapter 14 of Facure (2022)\n\nAdvanced difference in differences, notebook\n\nReading: chapter 24 of Facure (2022), Roth et al. (2023), Chaisemartin and D’Haultfœuille (2022)\n\nSynthetic Control, notebook\n\nReading: Abadie (2021),chapter 15 of Facure (2022)\n\n\n\n\n\n\n\nReferences\n\nAbadie, Alberto. 2021. “Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects.” Journal of Economic Literature 59 (2): 391–425. https://doi.org/10.1257/jel.20191450.\n\n\nChaisemartin, Clément de, and Xavier D’Haultfœuille. 2022. “Two-way fixed effects and differences-in-differences with heterogeneous treatment effects: a survey.” The Econometrics Journal 26 (3): C1–30. https://doi.org/10.1093/ectj/utac017.\n\n\nFacure, Matheus. 2022. Causal Inference for the Brave and True. https://matheusfacure.github.io/python-causality-handbook/landing-page.html.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. CRC Press. https://theeffectbook.net/.\n\n\nRoth, Jonathan, Pedro H. C. Sant’Anna, Alyssa Bilinski, and John Poe. 2023. “What’s Trending in Difference-in-Differences? A Synthesis of the Recent Econometrics Literature.” Journal of Econometrics 235 (2): 2218–44. https://doi.org/https://doi.org/10.1016/j.jeconom.2023.03.008."
  },
  {
    "objectID": "matching.html#setting",
    "href": "matching.html#setting",
    "title": "Matching",
    "section": "Setting",
    "text": "Setting\n\nPotential outcomes \\((Y_0, Y_1)\\)\nTreatment \\(T\\)\nObserve \\(Y = Y_0(1-T) + T Y_1\\)\nCovariates \\(X\\)\nAssume conditional independence \\((Y_0,Y_1) \\perp T | X\\)\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\En{{\\mathbb{En}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\]"
  },
  {
    "objectID": "matching.html#why-not-regression",
    "href": "matching.html#why-not-regression",
    "title": "Matching",
    "section": "Why not regression?",
    "text": "Why not regression?\n\nAverage treatment effect \\[\nATE = \\int \\Er[Y|T=1,X=x] - \\Er[Y|T=0,X=x] dP(x)\n\\]\nRegression gives the best linear approximation to \\(\\Er[Y|T,X]\\), so why not just estimate linear regression \\[\nY_i = \\hat{\\alpha} T_i + X_i'\\hat{\\beta} + \\hat{\\epsilon}_i\n\\] and, and then use \\(\\hat{\\alpha}\\) as an estimate of the ATE?"
  },
  {
    "objectID": "matching.html#why-not-regression-1",
    "href": "matching.html#why-not-regression-1",
    "title": "Matching",
    "section": "Why not regression?",
    "text": "Why not regression?\n\nPartial out (Frish-Waugh-Lovell theorem) \\[\n\\begin{align*}\n\\hat{\\alpha} = & \\frac{\\frac{1}{n} \\sum_{i=1}^n Y_i (T_i - X_i'(X'X)^{-1}X'T)}\n  {\\frac{1}{n} \\sum_{i=1}^n (T_i - X_i'(X'X)^{-1}X'T)^2} \\\\\n  \\inprob & \\Er\\left[Y_i \\underbrace{\\frac{T_i - X_i'\\pi}{\\Er[(T_i - X_i'\\pi)^2]}}_{\\equiv \\omega(T_i,X_i)}\\right] \\\\\n  = & \\Er\\left[Y_{0,i} \\omega(T_i,X_i)\\right] + \\Er\\left[(Y_{1,i}-Y_{0,i}) \\omega(T_i,X_i)T_i\\right]\n\\end{align*}\n\\] where \\(\\pi = \\argmin_{\\tilde{\\pi}} \\Er[(T_i - X_i'\\tilde{\\pi})^2]\\)\nNote: \\(\\Er[\\omega(T,X)] = 0\\), \\(\\Er[T\\omega(T,X)] = 1\\)"
  },
  {
    "objectID": "matching.html#why-not-regression-2",
    "href": "matching.html#why-not-regression-2",
    "title": "Matching",
    "section": "Why not regression?",
    "text": "Why not regression?\n\n\\(\\plim \\hat{\\alpha} = \\Er\\left[Y_{0,i} \\omega(T_i,X_i)\\right] + \\Er\\left[(Y_{1,i}-Y_{0,i}) \\omega(T_i,X_i)T_i\\right]\\)\nWhat can be in the range of \\(\\omega(T,X) = \\frac{T - X'\\pi}{\\Er[(T_i - X_i'\\pi)^2]}\\)?"
  },
  {
    "objectID": "matching.html#why-not-regression-3",
    "href": "matching.html#why-not-regression-3",
    "title": "Matching",
    "section": "Why not regression?",
    "text": "Why not regression?\n\n\nimports\nimport numpy as np\nfrom matplotlib import style\nfrom matplotlib import pyplot as plt\nstyle.use(\"fivethirtyeight\")\n\n\n\nnp.random.seed(1234)\n\ndef simulate(n, pi=np.array([0,1])):\n    X = np.random.randn(n, len(pi))\n    X[:,0] = 1\n    T = 1*((X @ pi + np.random.randn(n))&gt;0)\n    y0 = np.random.randn(n)\n    y1 = np.exp(3*(X[:,1]-2)) + np.random.randn(n)\n    y = T*y1 + (1-T)*y0\n    return(X,T,y,y0,y1)\n\nX,T,y,y0,y1 = simulate(1000)\n\npihat = np.linalg.solve(X.T @ X, X.T @ T)\nw = T - X @ pihat\nw = w/np.mean(w**2);"
  },
  {
    "objectID": "matching.html#why-not-regression-4",
    "href": "matching.html#why-not-regression-4",
    "title": "Matching",
    "section": "Why not regression?",
    "text": "Why not regression?\n\nTX = np.hstack((T.reshape(len(T),1),X))\nabhat = np.linalg.solve(TX.T @ TX, TX.T @ y)\nahat = abhat[0]\nprint(ahat)\n\n-0.06414016921951447\n\n\n\nnp.mean(y1-y0)\n\n0.22383059765273303\n\n\n\nWeights, \\(\\omega(T,X)\\), are not all positive, so the regression estimate can be negative even if \\(\\Er[Y_1 | X] - \\Er[Y_0|X]\\) is positive everywhere"
  },
  {
    "objectID": "matching.html#why-not-regression-5",
    "href": "matching.html#why-not-regression-5",
    "title": "Matching",
    "section": "Why not regression?",
    "text": "Why not regression?\n\n\nplot\nimport matplotlib.cm as cm\nfig, axes = plt.subplots(2, 1, figsize=(6, 6))\n\n# Create a scatter plot for the first panel (left)\naxes[0].scatter(X[:,1], w, c=T, cmap=cm.Dark2)\naxes[0].set_xlabel(\"X\")\naxes[0].set_ylabel(\"ωT\")\naxes[0].set_title(\"Weights\")\n\naxes[1].scatter(X[:,1], y1-y0, label=\"TE\")\naxes[1].set_xlabel(\"X\")\naxes[1].set_ylabel(\"Y₁ - Y₀\")\naxes[1].set_title(\"Treatment effects\")\n\n\n# Display the plot\nplt.tight_layout()  # Ensure proper layout spacing\nplt.show()"
  },
  {
    "objectID": "matching.html#matching-1",
    "href": "matching.html#matching-1",
    "title": "Matching",
    "section": "Matching",
    "text": "Matching\n\nIf not regression, then what? \\[\nATE = \\int \\Er[Y|T=1,X=x] - \\Er[Y|T=0,X=x] dP(x)\n\\]"
  },
  {
    "objectID": "matching.html#propensity-score",
    "href": "matching.html#propensity-score",
    "title": "Matching",
    "section": "Propensity Score",
    "text": "Propensity Score\n\nLet \\(e(X) = P(T=1|X=X)\\)\nNote: \\[\n\\begin{align*}\n\\Er[Y|X,T=1] - \\Er[Y|X,T=0] = & E\\left[\\frac{Y T}{e(X)}|X \\right] - E\\left[\\frac{Y(1-T)}{1-e(X)}|X \\right] \\\\\n= & E\\left[ Y \\frac{T - e(X)}{e(X)(1-e(X))} | X \\right]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "matching.html#inverse-probability-weighting",
    "href": "matching.html#inverse-probability-weighting",
    "title": "ECON526: Quantitative Economics with Data Science Applications",
    "section": "Inverse probability weighting",
    "text": "Inverse probability weighting\n\nEstimator \\[\n\\widehat{ATE}^{IPW} = \\frac{1}{n} \\sum_{i=1}^n \\frac{Y_iT_i}{\\hat{e}(X_i)} - \\frac{Y_i(1-T_i)}{1-\\hat{e}(X_i)}\n\\] where \\(\\hat{e}(X)\\) is some flexible estimator for \\(P(T=1|X)\\)\nDownside:\n\nDifficult statistical properties — choice of tuning parameters, strong assumptions needed"
  },
  {
    "objectID": "matching.html#doubly-robust-estimator",
    "href": "matching.html#doubly-robust-estimator",
    "title": "Matching",
    "section": "Doubly Robust Estimator",
    "text": "Doubly Robust Estimator\n\nEstimator \\[\n\\begin{align*}\n\\widehat{ATE}^{DR} = & \\frac{1}{n} \\sum_{i=1}^n \\hat{E}[Y|T=1,X=X_i] - \\hat{E}[Y|T=0,X=X_i] + \\\\\n& + \\frac{1}{n} \\sum_{i=1}^n  \\frac{T_i(Y_i - \\hat{E}[Y|T=1,X=X_i])}{\\hat{e}(X_i)} - \\\\\n& - \\frac{(1-T_i)(Y_i - \\hat{E}[Y|T=0,X=X_i])} {1-\\hat{e}(X_i)}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "matching.html#software",
    "href": "matching.html#software",
    "title": "Matching",
    "section": "Software",
    "text": "Software\n\nAdvice: use the doubly robust estimator with nonparametric estimates for \\(\\hat{E}[Y|T,X]\\) and \\(\\hat{e}(X)\\)\nRecommended package:\n\neconml has the correct estimator and examples of using it with nonparametric estimates\n\nfocuses on conditional instead of unconditional average treatment effects, but can be used for both"
  },
  {
    "objectID": "matching.html#example",
    "href": "matching.html#example",
    "title": "ECON526: Quantitative Economics with Data Science Applications",
    "section": "Example",
    "text": "Example\n\nfrom econml.dr import DRLearner, LinearDRLearner, SparseLinearDRLearner\nfrom econml.sklearn_extensions.linear_model import StatsModelsLinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.linear_model import LassoCV, LogisticRegressionCV, ElasticNetCV\nfrom sklearn.preprocessing import PolynomialFeatures#\n\nest = LinearDRLearner(featurizer=PolynomialFeatures(degree=20, include_bias=False),\n                model_regression=LassoCV(),\n                model_propensity=LogisticRegressionCV(),\n                #model_final=StatsModelsLinearRegression(),\n                cv=10)\nest.fit(y, T, X=None, W=X)\n\n&lt;econml.dr._drlearner.LinearDRLearner at 0x7face555b190&gt;\n\n\n\npoint = est.const_marginal_effect(None)\nlb, ub = est.const_marginal_effect_interval(None, alpha=0.05)\ndisplay(lb,point,ub)\n\narray([[-0.03922896]])\n\n\narray([[0.12089912]])\n\n\narray([[0.28102719]])"
  },
  {
    "objectID": "matching.html#references",
    "href": "matching.html#references",
    "title": "Matching",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nAbadie, Alberto, and Guido W. Imbens. 2008. “On the Failure of the Bootstrap for Matching Estimators.” Econometrica 76 (6): 1537–57. https://doi.org/https://doi.org/10.3982/ECTA6474.\n\n\nAthey, Susan, and Stefan Wager. 2019. “Estimating Treatment Effects with Causal Forests: An Application.”\n\n\nBorusyak, Kirill, and Xavier Jaravel. 2018. “Revisiting Event Study Designs.” https://scholar.harvard.edu/files/borusyak/files/borusyak_jaravel_event_studies.pdf.\n\n\nFacure, Matheus. 2022. Causal Inference for the Brave and True. https://matheusfacure.github.io/python-causality-handbook/landing-page.html.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. CRC Press. https://theeffectbook.net/.\n\n\nYeager, David S., Paul Hanselman, Gregory M. Walton, Jared S. Murray, Robert Crosnoe, Chandra Muller, Elizabeth Tipton, et al. 2019. “A National Experiment Reveals Where a Growth Mindset Improves Achievement.” Nature 573 (7774): 364–69. https://doi.org/10.1038/s41586-019-1466-y."
  },
  {
    "objectID": "matching.html#why-not-regression-6",
    "href": "matching.html#why-not-regression-6",
    "title": "ECON526: Quantitative Economics with Data Science Applications",
    "section": "Why not regression?",
    "text": "Why not regression?\n\n\nplot\nfig, axes = plt.subplots(2, 1, figsize=(8, 6))\n\n# Create a scatter plot for the first panel (left)\naxes[0].scatter(X[:,1], w*T)\naxes[0].set_xlabel(\"X\")\naxes[0].set_ylabel(\"ωT\")\naxes[0].set_title(\"Weights\")\n\naxes[1].scatter(X[:,1], y1-y0, label=\"TE\")\naxes[1].set_xlabel(\"X\")\naxes[1].set_ylabel(\"Y₁ - Y₀\")\naxes[1].set_title(\"Treatment effects\")\n\n\n# Display the plot\nplt.tight_layout()  # Ensure proper layout spacing\nplt.show()"
  },
  {
    "objectID": "matching.html#plug-in-estimator",
    "href": "matching.html#plug-in-estimator",
    "title": "Matching",
    "section": "Plug-in estimator",
    "text": "Plug-in estimator\n\nPlug in estimator: \\[\n\\widehat{ATE} = \\frac{1}{n} \\sum_{i=1}^n \\left(\\hat{E}[Y|T=1,X=X_i] - \\hat{E}[Y|T=0,X=X_i] \\right)\n\\] where \\(\\hat{E}[Y|T,X]\\) is some flexible estimator for \\(\\Er[Y|T,X]\\)\n\nif \\(X\\) is discrete, \\(\\hat{E}\\) can be conditional averages or equivalently, “saturated” regression\nif \\(X\\) continuous, \\(\\hat{E}\\) can be some nonparametric regression estimator\nOriginal approaches to this problem used nearest neighbor matching to estimate \\(\\hat{E}[Y|T,X]\\)\n\nDownside:\n\nDifficult statistical properties — choice of tuning parameters, strong assumptions needed, failure of bootstrap for nearest neighbors Abadie and Imbens (2008)"
  },
  {
    "objectID": "matching.html#doubly-robust-estimator-1",
    "href": "matching.html#doubly-robust-estimator-1",
    "title": "Matching",
    "section": "Doubly Robust Estimator",
    "text": "Doubly Robust Estimator\n\nDoubly robust in that:\n\nConsistent as long as either \\(\\hat{e}(X) \\inprob e(X)\\) or \\(\\hat{E}[Y|T,X] \\inprob \\Er[Y|T,X]\\)\nInsensitive to small changes in \\(\\hat{e}(X)\\) or \\(\\hat{E}[Y|T,X]\\)\n\nAllows: nicer statistical properties\n\nWeaker assumptions needed\nAsymptotic distribution is the same as if \\(e(X)\\) and \\(\\Er[Y|T,X]\\) were known"
  },
  {
    "objectID": "matching.html#example-in-simulation",
    "href": "matching.html#example-in-simulation",
    "title": "ECON526: Quantitative Economics with Data Science Applications",
    "section": "Example: in simulation",
    "text": "Example: in simulation\n\nInfeasible estimator: average of \\(Y_1 - Y_0\\)\n\n\nse = np.sqrt(np.var(y1-y0)/len(y1))\nate = np.mean(y1-y0)\ndisplay(ate-1.96*se, ate, ate+1.96*se)\n\n0.10010844404085255\n\n\n0.22383059765273303\n\n\n0.3475527512646135\n\n\n\nfrom econml.dr import DRLearner, LinearDRLearner, SparseLinearDRLearner\nfrom econml.sklearn_extensions.linear_model import StatsModelsLinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.linear_model import LassoCV, LogisticRegressionCV, ElasticNetCV\nfrom sklearn.preprocessing import PolynomialFeatures#\n\nest = LinearDRLearner(featurizer=PolynomialFeatures(degree=20, include_bias=False),\n                model_regression=LassoCV(),\n                model_propensity=LogisticRegressionCV(),\n                #model_final=StatsModelsLinearRegression(),\n                cv=10)\nest.fit(y, T, X=None, W=X)\n\n&lt;econml.dr._drlearner.LinearDRLearner at 0x7fae4bf8d250&gt;\n\n\n\npoint = est.const_marginal_effect(None)\nlb, ub = est.const_marginal_effect_interval(None, alpha=0.05)\ndisplay(lb,point,ub)\n\narray([[-0.03922896]])\n\n\narray([[0.12089912]])\n\n\narray([[0.28102719]])"
  },
  {
    "objectID": "matching.html#sources-and-further-reading",
    "href": "matching.html#sources-and-further-reading",
    "title": "Matching",
    "section": "Sources and Further Reading",
    "text": "Sources and Further Reading\n\nUseful additional reading is chapters 10-12 of Facure (2022) and chapter 14 of Huntington-Klein (2021).1\nThe representation of the estimate from a linear model as a weighted average is based on Borusyak and Jaravel (2018)\nThe growth mindset example is take from Facure (2022)\n\nThese slides do not mention the importance of overlap/balance, but hopefully I emphasized it during lecture. Overlap is very important in practice. The reading, especially Huntington-Klein (2021), cover it pretty well."
  },
  {
    "objectID": "matching.html#example-simulation",
    "href": "matching.html#example-simulation",
    "title": "Matching",
    "section": "Example: simulation",
    "text": "Example: simulation\n\nInfeasible estimator: average of \\(Y_1 - Y_0\\)\n\n\nse = np.sqrt(np.var(y1-y0)/len(y1))\nate = np.mean(y1-y0)\nprint(ate-1.96*se, ate, ate+1.96*se)\n\n0.10010844404085255 0.22383059765273303 0.3475527512646135"
  },
  {
    "objectID": "matching.html#example-simulation-1",
    "href": "matching.html#example-simulation-1",
    "title": "Matching",
    "section": "Example: simulation",
    "text": "Example: simulation\n\nfrom econml.dr import DRLearner, LinearDRLearner, SparseLinearDRLearner\nfrom econml.sklearn_extensions.linear_model import StatsModelsLinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.linear_model import LassoCV, LogisticRegressionCV, ElasticNetCV\nfrom sklearn.preprocessing import PolynomialFeatures#\n\nest = LinearDRLearner(featurizer=PolynomialFeatures(degree=20, include_bias=False),\n                model_regression=LassoCV(),\n                model_propensity=LogisticRegressionCV(),\n                #model_final=StatsModelsLinearRegression(),\n                cv=10)\nest.fit(y, T, X=None, W=X)\n\n&lt;econml.dr._drlearner.LinearDRLearner at 0x7f4d353b5250&gt;\n\n\n\npoint = est.const_marginal_effect(None)\nlb, ub = est.const_marginal_effect_interval(None, alpha=0.05)\nprint(lb,point,ub)\n\n[[-0.03922896]] [[0.12089912]] [[0.28102719]]"
  },
  {
    "objectID": "matching.html#national-study-of-learning-mindsets",
    "href": "matching.html#national-study-of-learning-mindsets",
    "title": "Matching",
    "section": "National Study of Learning Mindsets",
    "text": "National Study of Learning Mindsets\n\nOriginal study by Yeager et al. (2019)\nSynthetic data created by Athey and Wager (2019), downloaded from Facure (2022)"
  },
  {
    "objectID": "matching.html#data",
    "href": "matching.html#data",
    "title": "Matching",
    "section": "Data",
    "text": "Data\n\n\nimports\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import style\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as smf\nfrom causalinference import CausalModel\nstyle.use(\"fivethirtyeight\")\npd.set_option(\"display.max_columns\", 20)\ndatadir=\"./data\"\n\n\n\ndata = pd.read_csv(datadir+\"/learning_mindset.csv\")\ndata.sample(5, random_state=431)\n\n\n\n\n\n\n\n\nschoolid\nintervention\nachievement_score\nsuccess_expect\nethnicity\ngender\nfrst_in_family\nschool_urbanicity\nschool_mindset\nschool_achievement\nschool_ethnic_minority\nschool_poverty\nschool_size\n\n\n\n\n9366\n9\n0\n1.137192\n6\n1\n1\n1\n4\n1.324323\n-1.311438\n1.930281\n0.281143\n0.362031\n\n\n7810\n27\n0\n-0.554268\n5\n2\n1\n1\n1\n0.240267\n-0.785287\n0.611807\n0.612568\n-0.116284\n\n\n7532\n29\n0\n-0.462576\n6\n1\n1\n1\n1\n-0.373087\n0.113096\n-0.833417\n-1.924778\n-1.147314\n\n\n10381\n1\n0\n-0.402644\n5\n2\n2\n1\n3\n1.185986\n-1.129889\n1.009875\n1.005063\n-1.174702\n\n\n1244\n57\n1\n1.528680\n6\n4\n1\n1\n2\n0.097162\n-0.292353\n-1.030865\n-0.813799\n0.184716"
  },
  {
    "objectID": "matching.html#evidence-of-confounding",
    "href": "matching.html#evidence-of-confounding",
    "title": "Matching",
    "section": "Evidence of Confounding",
    "text": "Evidence of Confounding\n\n\nCode\ndef std_error(x):\n    return np.std(x, ddof=1) / np.sqrt(len(x))\n\ngrouped = data.groupby('success_expect')['intervention'].agg(['mean', std_error])\ngrouped = grouped.reset_index()\n\nfig, ax = plt.subplots()\nplt.errorbar(grouped['success_expect'],grouped['mean'],yerr=1.96*grouped['std_error'],fmt=\"o\")\nax.set_xlabel('student expectation of success')\nax.set_ylabel('P(treatment)')\nplt.show()"
  },
  {
    "objectID": "matching.html#unadjusted-estimate-of-ate",
    "href": "matching.html#unadjusted-estimate-of-ate",
    "title": "Matching",
    "section": "Unadjusted estimate of ATE",
    "text": "Unadjusted estimate of ATE\n\nprint(smf.ols(\"achievement_score ~ intervention\", data=data).fit().summary().tables[1])\n\n================================================================================\n                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept       -0.1538      0.012    -13.201      0.000      -0.177      -0.131\nintervention     0.4723      0.020     23.133      0.000       0.432       0.512\n================================================================================\n\n\n\nprint(smf.ols(\"achievement_score ~ intervention\", data=data).fit(\n    cov_type=\"cluster\", cov_kwds={'groups': data['schoolid']}).summary().tables[1])\n\n================================================================================\n                   coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept       -0.1538      0.036     -4.275      0.000      -0.224      -0.083\nintervention     0.4723      0.025     19.184      0.000       0.424       0.521\n================================================================================"
  },
  {
    "objectID": "matching.html#regression-estimate-of-ate",
    "href": "matching.html#regression-estimate-of-ate",
    "title": "Matching",
    "section": "Regression estimate of ATE",
    "text": "Regression estimate of ATE\n\nols = smf.ols(\"achievement_score ~ intervention + success_expect + ethnicity + gender + frst_in_family + school_urbanicity + school_mindset + school_achievement + school_ethnic_minority + school_poverty + school_size\",data=data).fit()\nprint(ols.summary().tables[1])\n\n==========================================================================================\n                             coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------------\nIntercept                 -1.7786      0.056    -31.880      0.000      -1.888      -1.669\nintervention               0.3964      0.018     22.192      0.000       0.361       0.431\nsuccess_expect             0.3746      0.008     49.514      0.000       0.360       0.389\nethnicity                  0.0043      0.002      2.049      0.040       0.000       0.008\ngender                    -0.2684      0.017    -16.060      0.000      -0.301      -0.236\nfrst_in_family            -0.1310      0.018     -7.248      0.000      -0.166      -0.096\nschool_urbanicity          0.0573      0.007      8.240      0.000       0.044       0.071\nschool_mindset            -0.1484      0.011    -13.083      0.000      -0.171      -0.126\nschool_achievement        -0.0253      0.013     -1.902      0.057      -0.051       0.001\nschool_ethnic_minority     0.1197      0.011     11.178      0.000       0.099       0.141\nschool_poverty            -0.0154      0.011     -1.466      0.143      -0.036       0.005\nschool_size               -0.0467      0.011     -4.326      0.000      -0.068      -0.026\n=========================================================================================="
  },
  {
    "objectID": "matching.html#regression-estimate-of-ate-weights",
    "href": "matching.html#regression-estimate-of-ate-weights",
    "title": "Matching",
    "section": "Regression estimate of ATE: weights",
    "text": "Regression estimate of ATE: weights\n\nlpm = smf.ols(\"intervention ~ success_expect + ethnicity + gender + frst_in_family + school_urbanicity + school_mindset + school_achievement + school_ethnic_minority + school_poverty + school_size\",data=data).fit(cov_type=\"cluster\", cov_kwds={'groups': data['schoolid']})\nw = lpm.resid / np.var(lpm.resid)\nprint(np.mean(data.achievement_score*w))\n\n0.39640236033389553"
  },
  {
    "objectID": "matching.html#propensity-score-matching",
    "href": "matching.html#propensity-score-matching",
    "title": "Matching",
    "section": "Propensity Score Matching",
    "text": "Propensity Score Matching\n\ncateg = [\"ethnicity\", \"gender\", \"school_urbanicity\",\"success_expect\"]\ncont = [\"school_mindset\", \"school_achievement\", \"school_ethnic_minority\", \"school_poverty\", \"school_size\"]\n\ndata_with_categ = pd.concat([\n    data.drop(columns=categ), # dataset without the categorical features\n    pd.get_dummies(data[categ], columns=categ, drop_first=False)# categorical features converted to dummies\n], axis=1)\n\nprint(data_with_categ.shape)\nT = 'intervention'\nY = 'achievement_score'\nX = data_with_categ.columns.drop(['schoolid', T, Y])\n\n(10391, 38)"
  },
  {
    "objectID": "matching.html#inverse-propensity-weighting",
    "href": "matching.html#inverse-propensity-weighting",
    "title": "Matching",
    "section": "Inverse propensity weighting",
    "text": "Inverse propensity weighting\n\nEstimator \\[\n\\widehat{ATE}^{IPW} = \\frac{1}{n} \\sum_{i=1}^n \\frac{Y_iT_i}{\\hat{e}(X_i)} - \\frac{Y_i(1-T_i)}{1-\\hat{e}(X_i)}\n\\] where \\(\\hat{e}(X)\\) is some flexible estimator for \\(P(T=1|X)\\)\nDownside:\n\nDifficult statistical properties — choice of tuning parameters, strong assumptions needed"
  },
  {
    "objectID": "matching.html#doubly-robust",
    "href": "matching.html#doubly-robust",
    "title": "Matching",
    "section": "Doubly Robust",
    "text": "Doubly Robust\n\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.linear_model import LassoCV, LogisticRegressionCV, ElasticNetCV\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndef robustate(T,Y,X,psmodel=LogisticRegressionCV(),ymodel=LassoCV(), cluster=None):\n    pfit = psmodel.fit(X,T)\n    ps = pfit.predict_proba(X)[:,1]\n    ey1fit = ymodel.fit(X[T==1],Y[T==1])\n    ey0fit = sklearn.base.clone(ymodel).fit(X[T==0],Y[T==0])\n    ey1 = ey1fit.predict(X)\n    ey0 = ey0fit.predict(X)\n    ate_terms = ey1 - ey0 + T*(Y- ey1)/ps - (1-T)*(Y-ey0)/(1-ps)\n    ate = np.mean(ate_terms)\n    # check if cluster is None\n    if cluster is None :\n        ate_se = np.sqrt(np.var(ate_terms)/len(ate_terms))\n    else :\n        creg=smf.ols(\"y ~ 1\", pd.DataFrame({\"y\" : ate_terms})).fit(cov_type=\"cluster\", cov_kwds={'groups': cluster})\n        ate_se = np.sqrt(creg.cov_params().iloc[0,0])\n\n    return(ate, ate_se, ps, ey1,ey0)\n\nate,se,ps,ey1,ey0 = robustate(data_with_categ[T],data_with_categ[Y],data_with_categ[X],cluster=data_with_categ['schoolid'])\nprint(ate-1.96*se, ate, ate+1.96*se)\n\n0.33214226035254746 0.3836157943394324 0.4350893283263173\n\n\n1\nWe have glossed over some details needed for doubly robust estimation to have nice statistical properties. Those details matter and are not implemented correctly above. It is better to use the econml instead."
  },
  {
    "objectID": "matching.html#doubly-robust-1",
    "href": "matching.html#doubly-robust-1",
    "title": "Matching",
    "section": "Doubly Robust",
    "text": "Doubly Robust\n\nbetter to use the econml package\n\n\nfrom econml.dr import DRLearner, LinearDRLearner, SparseLinearDRLearner\n\nest = LinearDRLearner(#featurizer=PolynomialFeatures(degree=2, include_bias=False),\n                model_regression=LassoCV(),\n                model_propensity=LogisticRegressionCV(),\n                cv=5)\n\nest.fit(data_with_categ[Y], data_with_categ[T], X=None, W=data_with_categ[X])\npoint = est.const_marginal_effect(None)\nlb, ub = est.const_marginal_effect_interval(None, alpha=0.05)\nprint(lb,point,ub)\n\n[[0.35515763]] [[0.38856831]] [[0.42197899]]"
  },
  {
    "objectID": "matching.html#propensity-score-1",
    "href": "matching.html#propensity-score-1",
    "title": "Matching",
    "section": "Propensity Score",
    "text": "Propensity Score\n\nso \\[\nATE = \\Er\\left[ \\frac{Y T}{e(X)} -  \\frac{Y(1-T)}{1-e(X)}\\right] = \\Er\\left[ Y \\frac{T - e(X)}{e(X)(1-e(X))} \\right]\n\\]"
  },
  {
    "objectID": "matching.html#software-1",
    "href": "matching.html#software-1",
    "title": "Matching",
    "section": "Software",
    "text": "Software\n\nOther packages:\n\ncausalinference has a double robust estimator, but it estimates \\(\\hat{E}[Y|T,X]\\) via linear regression and \\(\\hat{e}(X)\\) via logit (maybe probit, not sure)\n\ncan make nonparametric by adding e.g. powers of \\(x\\) to \\(X\\), but need to manage manually\n\nzEpid is similiar to causalinference, but has a formula interface, so slightly easier to make model more flexible"
  },
  {
    "objectID": "matching.html#unadjusted-estimate-of-ate-1",
    "href": "matching.html#unadjusted-estimate-of-ate-1",
    "title": "Matching",
    "section": "Unadjusted estimate of ATE",
    "text": "Unadjusted estimate of ATE\n\n\nCode\nfig,ax=plt.subplots()\nplt.hist(data.query(\"intervention==0\")[\"achievement_score\"], bins=20, alpha=0.3, color=\"C2\")\nplt.hist(data.query(\"intervention==1\")[\"achievement_score\"], bins=20, alpha=0.3, color=\"C3\")\nplt.vlines(-0.1538, 0, 300, label=\"Untreated\", color=\"C2\")\nplt.vlines(-0.1538+0.4723, 0, 300, label=\"Treated\", color=\"C3\")\nax.set_xlabel(\"Achievement Score\")\nax.set_ylabel(\"N\")\nplt.legend()\nplt.show();"
  },
  {
    "objectID": "matching.html#regression-estimate-of-ate-weights-1",
    "href": "matching.html#regression-estimate-of-ate-weights-1",
    "title": "Matching",
    "section": "Regression estimate of ATE: weights",
    "text": "Regression estimate of ATE: weights\n\n\nCode\nfig,ax=plt.subplots()\nplt.hist(w[data.intervention==0], bins=20, alpha=0.3, color=\"C2\", label=\"Untreated\")\nplt.hist(w[data.intervention==1], bins=20, alpha=0.3, color=\"C3\", label=\"Treated\")\nax.set_xlabel(\"w\")\nax.set_ylabel(\"N\")\nplt.legend()\nplt.show();"
  },
  {
    "objectID": "matching.html#propensity-score-matching-1",
    "href": "matching.html#propensity-score-matching-1",
    "title": "Matching",
    "section": "Propensity Score Matching",
    "text": "Propensity Score Matching\n\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.neighbors import KNeighborsRegressor\nimport sklearn\n\ndef propensitymatching(T,Y,X,psmodel=LogisticRegressionCV(),neighbormodel=KNeighborsRegressor(n_neighbors=1,algorithm='auto',weights='uniform')):\n    pfit = psmodel.fit(X,T)\n    ps = pfit.predict_proba(X)[:,1]\n    ey1 = neighbormodel.fit(ps[T==1].reshape(-1,1),Y[T==1])\n    ey0 = sklearn.base.clone(neighbormodel).fit(ps[T==0].reshape(-1,1),Y[T==0])\n    tex = ey1.predict(ps.reshape(-1,1)) - ey0.predict(ps.reshape(-1,1))\n    ate = np.mean(tex)\n    return(ate, tex,ps)\n\nate,tex,ps=propensitymatching(data_with_categ[T],data_with_categ[Y],data_with_categ[X])\nprint(ate)\n\n0.3423570909254285"
  },
  {
    "objectID": "matching.html#propensity-score-matching-2",
    "href": "matching.html#propensity-score-matching-2",
    "title": "Matching",
    "section": "Propensity Score Matching",
    "text": "Propensity Score Matching\n\n\nCode\nfig, ax = plt.subplots(2,1)\ntreat = data.intervention\nax[0].scatter(ps[treat==0],tex[treat==0],color=\"C2\")\nax[0].scatter(ps[treat==1],tex[treat==1],color=\"C3\")\nax[1].hist(ps[treat==0],bins=20,color=\"C2\",label=\"Untreated\")\nax[1].hist(ps[treat==1],bins=20,color=\"C3\",label=\"Treated\")\nax[1].set_xlabel(\"P(Treatment)\")\nax[1].set_ylabel(\"N\")\nax[0].set_ylabel(\"E[Y|T=1,P(X)] - E[Y|T=0,P(X)]\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "matching.html#inverse-propensity-weighting-1",
    "href": "matching.html#inverse-propensity-weighting-1",
    "title": "Matching",
    "section": "Inverse Propensity Weighting",
    "text": "Inverse Propensity Weighting\n\ndef ipw(T,Y,X,psmodel=LogisticRegressionCV()):\n    pfit = psmodel.fit(X,T)\n    ps = pfit.predict_proba(X)[:,1]\n    ate=np.mean(Y*(T - ps)/(ps*(1-ps)))\n    return(ate,ps)\n\nate,ps = ipw(data_with_categ[T],data_with_categ[Y],data_with_categ[X])\nprint(ate)\n\n0.46498505452715805"
  },
  {
    "objectID": "did.html#introduction-1",
    "href": "did.html#introduction-1",
    "title": "Introduction to Difference in Differences",
    "section": "Introduction",
    "text": "Introduction\n\nHave some policy applied to some observations but not others, and observe outcome before and after policy\nIdea: compare outcome before and after policy in treated and untreated group\nChange in outcome in treated group reflects both effect of policy and time trend, change in untreated group captures time trend"
  },
  {
    "objectID": "did.html#example-impact-of-billboards",
    "href": "did.html#example-impact-of-billboards",
    "title": "Introduction to Difference in Differences",
    "section": "Example: Impact of Billboards",
    "text": "Example: Impact of Billboards\n\nFrom Facure (2022) chapter 13\nBank placed billboards advertising savings accounts in Porto Alegre in July\nData on deposits in May and July in Porto Alegre and Florianopolis"
  },
  {
    "objectID": "did.html#example-impact-of-billboards-1",
    "href": "did.html#example-impact-of-billboards-1",
    "title": "Introduction to Difference in Differences",
    "section": "Example: Impact of Billboards",
    "text": "Example: Impact of Billboards\n\n\nCode\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import style\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as smf\n\ndatadir=\"./data\"\n\n\n\ndata = pd.read_csv(datadir + \"/billboard_impact.csv\")\ndata.head()\n\n\n\n\n\n\n\n\ndeposits\npoa\njul\n\n\n\n\n0\n42\n1\n0\n\n\n1\n0\n1\n0\n\n\n2\n52\n1\n0\n\n\n3\n119\n1\n0\n\n\n4\n21\n1\n0"
  },
  {
    "objectID": "did.html#means-and-differences",
    "href": "did.html#means-and-differences",
    "title": "Introduction to Difference in Differences",
    "section": "Means and Differences",
    "text": "Means and Differences\n\ntbl = data.groupby(['jul','poa']).mean().unstack()\ntbl\n\n\n\n\n\n\n\n\ndeposits\n\n\npoa\n0\n1\n\n\njul\n\n\n\n\n\n\n0\n171.642308\n46.01600\n\n\n1\n206.165500\n87.06375\n\n\n\n\n\n\n\n\ntbl.diff(axis=0).iloc[1,:]\n\n          poa\ndeposits  0      34.523192\n          1      41.047750\nName: 1, dtype: float64\n\n\n\ntbl.diff(axis=1).iloc[:,1]\n\njul\n0   -125.626308\n1   -119.101750\nName: (deposits, 1), dtype: float64\n\n\n\ntbl.diff(axis=0).diff(axis=1).iloc[1,1]\n\n6.524557692307688"
  },
  {
    "objectID": "did.html#setup",
    "href": "did.html#setup",
    "title": "Introduction to Difference in Differences",
    "section": "Setup",
    "text": "Setup\n\nTwo periods, binary treatment in second period\nPotential outcomes \\(\\{y_{it}(0),y_{it}(1)\\}_{t=0}^1\\) for \\(i=1,...,N\\)\nTreatment \\(D_{it} \\in \\{0,1\\}\\),\n\n\\(D_{i0} = 0\\) \\(\\forall i\\)\n\\(D_{i1} = 1\\) for some, \\(0\\) for others\n\nObserve \\(y_{it} = y_{it}(0)(1-D_{it}) + D_{it} y_{it}(1)\\)"
  },
  {
    "objectID": "did.html#identification",
    "href": "did.html#identification",
    "title": "Introduction to Difference in Differences",
    "section": "Identification",
    "text": "Identification\n\nAverage treatment effect on the treated: \\[\n\\begin{align*}\nATT & = \\Er[y_{i1}(1) - \\color{red}{y_{i1}(0)} | D_{i1} = 1] \\\\\n& = \\Er[y_{i1}(1) - y_{i0}(0) | D_{i1} = 1] - \\Er[\\color{red}{y_{i1}(0)} - y_{i0}(0) | D_{i1}=1] \\\\\n& \\text{ assume } \\Er[\\color{red}{y_{i1}(0)} - y_{i0}(0) | D_{i1}=1] =  \\Er[y_{i1}(0) - y_{i0}(0) | D_{i1}=0] \\\\\n& = \\Er[y_{i1}(1) - y_{i0}(0) | D_{i1} = 1] - \\Er[y_{i1}(0) - y_{i0}(0) | D_{i1}=0] \\\\\n& = \\Er[y_{i1} - y_{i0} | D_{i1}=1, D_{i0}=0] - \\Er[y_{i1} - y_{i0} | D_{i1}=0, D_{i0}=0]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "did.html#important-assumptions",
    "href": "did.html#important-assumptions",
    "title": "Introduction to Difference in Differences",
    "section": "Important Assumptions",
    "text": "Important Assumptions\n\nNo anticipation: \\(D_{i1}=1\\) does not affect \\(y_{i0}\\)\n\nbuilt into the potential outcomes notation we used, relax by allowing potential outcomes given sequence of \\(D\\), i.e. \\(y_{it}(D_{i0},D_{i1})\\)\n\nParallel trends: \\(\\Er[\\color{red}{y_{i1}(0)} - y_{i0}(0) |D_{i1}=1,D_{i0}=0] = \\Er[y_{i1}(0) - y_{i0}(0) | D_{i1}=0], D_{i0}=0]\\)\n\nnot invariant to tranformations of \\(y\\)"
  },
  {
    "objectID": "did.html#estimation",
    "href": "did.html#estimation",
    "title": "Introduction to Difference in Differences",
    "section": "Estimation",
    "text": "Estimation\n\n\nPlugin: \\[\n\\widehat{ATT} = \\frac{ \\sum_{i=1}^n (y_{i1} - y_{i0})D_{i1}(1-D_{i0})}{\\sum_{i=1}^n D_{i1}(1-D_{i0})} -  \\frac{ \\sum_{i=1}^n (y_{i1} - y_{i0})(1-D_{i1})(1-D_{i0})}{\\sum_{i=1}^n (1-D_{i1})(1-D_{i0})}\n\\]\nRegression: \\[\ny_{it} = \\delta_t + \\alpha 1\\{D_{i1}=1\\} + \\beta D_{it} + \\epsilon_{it}\n\\] then \\(\\hat{\\beta} = \\widehat{ATT}\\)"
  },
  {
    "objectID": "did.html#visualizing-difference-in-differences",
    "href": "did.html#visualizing-difference-in-differences",
    "title": "Introduction to Difference in Differences",
    "section": "Visualizing Difference in Differences",
    "text": "Visualizing Difference in Differences\n\npoa_before = data.query(\"poa==1 & jul==0\")[\"deposits\"].mean()\npoa_after = data.query(\"poa==1 & jul==1\")[\"deposits\"].mean()\nfl_before = data.query(\"poa==0 & jul==0\")[\"deposits\"].mean()\nfl_after = data.query(\"poa==0 & jul==1\")[\"deposits\"].mean()\nplt.figure(figsize=(10,5))\nplt.plot([\"May\", \"Jul\"], [fl_before, fl_after], label=\"FL\", lw=2)\nplt.plot([\"May\", \"Jul\"], [poa_before, poa_after], label=\"POA\", lw=2)\n\nplt.plot([\"May\", \"Jul\"], [poa_before, poa_before+(fl_after-fl_before)],\n         label=\"Counterfactual\", lw=2, color=\"C2\", ls=\"-.\")\n\nplt.legend();"
  },
  {
    "objectID": "did.html#estimation-via-regression",
    "href": "did.html#estimation-via-regression",
    "title": "Introduction to Difference in Differences",
    "section": "Estimation via Regression",
    "text": "Estimation via Regression\n\nsmf.ols('deposits ~ poa*jul', data=data).fit().summary().tables[1]\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n171.6423\n2.363\n72.625\n0.000\n167.009\n176.276\n\n\npoa\n-125.6263\n4.484\n-28.015\n0.000\n-134.418\n-116.835\n\n\njul\n34.5232\n3.036\n11.372\n0.000\n28.571\n40.475\n\n\npoa:jul\n6.5246\n5.729\n1.139\n0.255\n-4.706\n17.755"
  },
  {
    "objectID": "did.html#further-topics",
    "href": "did.html#further-topics",
    "title": "Introduction to Difference in Differences",
    "section": "Further Topics",
    "text": "Further Topics\n\nMore periods, more groups\nCovariates\nPre-trends"
  },
  {
    "objectID": "did.html#reading",
    "href": "did.html#reading",
    "title": "Introduction to Difference in Differences",
    "section": "Reading",
    "text": "Reading\n\nChapter 13 of Facure (2022)"
  },
  {
    "objectID": "did.html#references",
    "href": "did.html#references",
    "title": "Introduction to Difference in Differences",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nFacure, Matheus. 2022. Causal Inference for the Brave and True. https://matheusfacure.github.io/python-causality-handbook/landing-page.html."
  },
  {
    "objectID": "fe.html#panel-data",
    "href": "fe.html#panel-data",
    "title": "Fixed Effects",
    "section": "Panel Data",
    "text": "Panel Data\n\nUnits \\(i=1,..., n\\)\n\nEx: people, firms, cities, countries\n\nTime \\(t=1,..., T\\)\nObserve \\(\\left\\{(y_{it}, X_{it})\\right\\}_{i=1,t=1}^{n,T}\\)"
  },
  {
    "objectID": "fe.html#linear-model",
    "href": "fe.html#linear-model",
    "title": "Fixed Effects",
    "section": "Linear Model",
    "text": "Linear Model\n\n\nModel \\[\ny_{it} = X_{it}'\\beta + \\overbrace{U_i'\\gamma + \\epsilon_{it}}^{\\text{unobserved}}\n\\]\n\nTime invariant confounders \\(U_i\\)\n\nSubtract individual averages \\[\n\\begin{align*}\ny_{it} - \\bar{y}_i & = (X_{it} - \\bar{X}_i)'\\beta + (\\epsilon_{it} -\n                   \\bar{\\epsilon}_i) \\\\\n\\ddot{y}_{it} & = \\ddot{X}_{it}' \\beta + \\ddot{\\epsilon}_{it}\n\\end{align*}\n\\]\nEquivalent to estimating with individual dummies \\[\ny_{it} = X_{it}'\\beta + \\alpha_i + \\epsilon_{it}\n\\]\n\n\n\n\nEliminates \\(U_i\\) and any time invariant observed \\(X_i\\)"
  },
  {
    "objectID": "fe.html#ols",
    "href": "fe.html#ols",
    "title": "Fixed Effects",
    "section": "OLS",
    "text": "OLS\n\n\nimports\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import style\nfrom matplotlib import pyplot as plt\nimport statsmodels.formula.api as smf\nstyle.use(\"fivethirtyeight\")\n\n\n1\n\n\nCode\ntoy_panel = pd.DataFrame({\n    \"mkt_costs\":[5,4,3.5,3, 10,9.5,9,8, 4,3,2,1, 8,7,6,4],\n    \"purchase\":[12,9,7.5,7, 9,7,6.5,5, 15,14.5,14,13, 11,9.5,8,5],\n    \"city\":[\"C0\",\"C0\",\"C0\",\"C0\", \"C2\",\"C2\",\"C2\",\"C2\", \"C1\",\"C1\",\"C1\",\"C1\", \"C3\",\"C3\",\"C3\",\"C3\"]\n})\n\nm = smf.ols(\"purchase ~ mkt_costs\", data=toy_panel).fit()\n\nplt.scatter(toy_panel.mkt_costs, toy_panel.purchase)\nplt.plot(toy_panel.mkt_costs, m.fittedvalues, c=\"C5\", label=\"Regression Line\")\nplt.xlabel(\"Marketing Costs (in 1000)\")\nplt.ylabel(\"In-app Purchase (in 1000)\")\nplt.title(\"Simple OLS Model\")\nplt.legend();\n\n\n\n\n\nCode from Facure (2022)"
  },
  {
    "objectID": "fe.html#fixed-effects-within",
    "href": "fe.html#fixed-effects-within",
    "title": "Fixed Effects",
    "section": "Fixed Effects / Within",
    "text": "Fixed Effects / Within\n1\n\n\nCode\nfe = smf.ols(\"purchase ~ mkt_costs + C(city)\", data=toy_panel).fit()\n\nfe_toy = toy_panel.assign(y_hat = fe.fittedvalues)\n\nplt.scatter(toy_panel.mkt_costs, toy_panel.purchase, c=toy_panel.city)\nfor city in fe_toy[\"city\"].unique():\n    plot_df = fe_toy.query(f\"city=='{city}'\")\n    plt.plot(plot_df.mkt_costs, plot_df.y_hat, c=\"C5\")\n\nplt.title(\"Fixed Effect Model\")\nplt.xlabel(\"Marketing Costs (in 1000)\")\nplt.ylabel(\"In-app Purchase (in 1000)\");\n\n\n\n\n\nCode from Facure (2022)"
  },
  {
    "objectID": "fe.html#large-n-small-t",
    "href": "fe.html#large-n-small-t",
    "title": "Fixed Effects",
    "section": "Large \\(n\\), Small \\(T\\)",
    "text": "Large \\(n\\), Small \\(T\\)\n\nOften \\(n&gt;&gt;T\\)\nUsual analysis of fixed effects uses asymptotics with \\(n \\to \\infty\\), \\(T\\) fixed\n\nWe will mostly stick to that, but if you have data with \\(n \\approx T\\), other approaches can be better"
  },
  {
    "objectID": "fe.html#strict-exogeneity",
    "href": "fe.html#strict-exogeneity",
    "title": "Fixed Effects",
    "section": "Strict Exogeneity",
    "text": "Strict Exogeneity\n\nIn fixed effect model \\[\ny_{it} - \\bar{y}_i  = (X_{it} - \\bar{X}_i)'\\beta + (\\epsilon_{it} - \\bar{\\epsilon}_i)\n\\] for \\(\\hat{\\beta}^{FE} \\inprob \\beta\\), need \\(\\Er[(X_{it} - \\bar{X}_i)(\\epsilon_{it} - \\bar{\\epsilon}_i)]=0\\)\nI.e. \\(\\Er[X_{it} \\epsilon_{is}] = 0\\) for all \\(t, s\\)"
  },
  {
    "objectID": "fe.html#strict-exogeneity-1",
    "href": "fe.html#strict-exogeneity-1",
    "title": "Fixed Effects",
    "section": "Strict Exogeneity",
    "text": "Strict Exogeneity\n\nProblematic with dynamics, e.g.\n\n\\(X_{it}\\) including lagged \\(y_{it-1}\\)\n\\(X_{it}\\) affected by past \\(y\\)\n“Nickell bias”\n\nSee Chen, Chernozhukov, and Fernández-Val (2019) for bias correction under weak exogeneity, \\(\\Er[X_{it} \\epsilon_{is}] = 0\\) for \\(t \\leq s\\)"
  },
  {
    "objectID": "fe.html#standard-errors",
    "href": "fe.html#standard-errors",
    "title": "Fixed Effects",
    "section": "Standard Errors",
    "text": "Standard Errors\n\nGenerally, good idea to use clustered standard errors, clustered on \\(i\\)\nSee MacKinnon, Nielsen, and Webb (2023) for guide to clustered standard errors"
  },
  {
    "objectID": "fe.html#sources-and-further-reading",
    "href": "fe.html#sources-and-further-reading",
    "title": "Fixed Effects",
    "section": "Sources and Further Reading",
    "text": "Sources and Further Reading\n\nFacure (2022) chapter 14\nHuntington-Klein (2021) chapter 16"
  },
  {
    "objectID": "fe.html#references",
    "href": "fe.html#references",
    "title": "Fixed Effects",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nChen, Shuowen, Victor Chernozhukov, and Iván Fernández-Val. 2019. “Mastering Panel Metrics: Causal Impact of Democracy on Growth.” AEA Papers and Proceedings 109 (May): 77–82. https://doi.org/10.1257/pandp.20191071.\n\n\nFacure, Matheus. 2022. Causal Inference for the Brave and True. https://matheusfacure.github.io/python-causality-handbook/landing-page.html.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. CRC Press. https://theeffectbook.net/.\n\n\nMacKinnon, James G., Morten Ørregaard Nielsen, and Matthew D. Webb. 2023. “Cluster-Robust Inference: A Guide to Empirical Practice.” Journal of Econometrics 232 (2): 272–99. https://doi.org/https://doi.org/10.1016/j.jeconom.2022.04.001."
  },
  {
    "objectID": "moredid.html#setup",
    "href": "moredid.html#setup",
    "title": "Difference in Differences II",
    "section": "Setup",
    "text": "Setup\n\nTwo Many periods, binary treatment in second some periods\nPotential outcomes \\(\\{y_{it}(0),y_{it}(1)\\}_{t=1}^T\\) for \\(i=1,...,N\\)\nTreatment \\(D_{it} \\in \\{0,1\\}\\),\n\n\\(D_{i0} = 0\\) \\(\\forall i\\)\n\\(D_{i1} = 1\\) for some, \\(0\\) for others\n\nObserve \\(y_{it} = y_{it}(0)(1-D_{it}) + D_{it} y_{it}(1)\\)"
  },
  {
    "objectID": "moredid.html#identification",
    "href": "moredid.html#identification",
    "title": "Difference in Differences II",
    "section": "Identification",
    "text": "Identification\n\nSame logic as before, \\[\n\\begin{align*}\nATT_{t,t-s} & = \\Er[y_{it}(1) - \\color{red}{y_{it}(0)} | D_{it} = 1, D_{it-s}=0] \\\\\n& = \\Er[y_{it}(1) - y_{it-s}(0) | D_{it} = 1, D_{it-s}=0] - \\\\\n& \\;\\; -  \\Er[\\color{red}{y_{it}(0)} - y_{t-s}(0) | D_{it}=1, D_{it-s}=0]\n\\end{align*}\n\\]\n\nassume \\(\\Er[\\color{red}{y_{it}(0)} - y_{it-s}(0) | D_{it}=1, D_{it-s}=0] = \\Er[y_{it}(0) - y_{it-s}(0) | D_{it}=0, D_{it-s}=0]\\)\n\n\n\\[\n\\begin{align*}\nATT_{t,t-s}& = \\Er[y_{it} - y_{it-s} | D_{it}=1, D_{it-s}=0] - \\Er[y_{it} - y_{it-s} | D_{it}=0, D_{it-s}=0]\n\\end{align*}\n\\] - Similarly, can identify various other interpretable average treatment effects conditional on being treated at some times and not others"
  },
  {
    "objectID": "moredid.html#estimation",
    "href": "moredid.html#estimation",
    "title": "Difference in Differences II",
    "section": "Estimation",
    "text": "Estimation\n\nPlugin\nFixed effects? \\[\ny_{it} = \\beta D_{it} + \\alpha_i + \\delta_t + \\epsilon_{it}\n\\] When will \\(\\hat{\\beta}^{FE}\\) consistently estimate some interpretable conditional average of treatment effects?"
  },
  {
    "objectID": "moredid.html#fixed-effects",
    "href": "moredid.html#fixed-effects",
    "title": "Difference in Differences II",
    "section": "Fixed Effects",
    "text": "Fixed Effects\n\nAs with matching, \\[\n\\begin{align*}\n\\hat{\\beta} = & \\sum_{i=1,t=1}^{n,T} y_{it} \\overbrace{\\frac{\\tilde{D}_{it}}{ \\sum_{i,t} \\tilde{D}_{it}^2 }}^{\\hat{\\omega}_{it}} = \\sum_{i=1,t=1}^{n,T} y_{it}(0) \\hat{\\omega}_{it} + \\sum_{i=1,t=1}^{n,T} D_{it} (y_{it}(1) - y_{it}(0)) \\hat{\\omega}_{it}\n\\end{align*}\n\\] where \\[\n\\begin{align*}\n\\tilde{D}_{it} & = D_{it} - \\frac{1}{n} \\sum_{j=1}^n (D_{jt} - \\frac{1}{T} \\sum_{s=1}^T D_{js}) - \\frac{1}{T} \\sum_{s=1}^T D_{is} \\\\\n& = D_{it} - \\frac{1}{n} \\sum_{j=1}^n D_{jt} - \\frac{1}{T} \\sum_{s=1}^T D_{is} + \\frac{1}{nT} \\sum_{j,s} D_{js}\n\\end{align*}\n\\]\n\n\n\nimports\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import style\nfrom matplotlib import pyplot as plt\nstyle.use(\"fivethirtyeight\")"
  },
  {
    "objectID": "moredid.html#weights",
    "href": "moredid.html#weights",
    "title": "Difference in Differences II",
    "section": "Weights",
    "text": "Weights\n\n\nCode\ndef assigntreat(n, T, portiontreated):\n    treated = np.zeros((n, T), dtype=bool)\n    for t in range(1, T):\n        treated[:, t] = treated[:, t - 1]\n        if portiontreated[t] &gt; 0:\n            treated[:, t] = np.logical_or(treated[:, t-1], np.random.rand(n) &lt; portiontreated[t])\n    return treated\n\ndef weights(D):\n    D̈ = D - np.mean(D, axis=0) - np.mean(D, axis=1)[:, np.newaxis] + np.mean(D)\n    ω = D̈ / np.sum(D̈**2)\n    return ω\n\nn = 100\nT = 9\npt = np.zeros(T)\npt[T//2 + 1] = 0.5\nD = assigntreat(n, T,pt)\ny = np.random.randn(n, T)\nweighted_sum = np.sum(y * weights(D))\nprint(weighted_sum)\n\n\n-0.21363520721210053\n\n\n\n\nCode\n# check that it matches fixed effect estimate from a package\nfrom linearmodels.panel import PanelOLS\n\ndf = pd.DataFrame({\n    'id': np.repeat(np.arange(1, n + 1), T),\n    't': np.tile(np.arange(1, T + 1), n),\n    'y': y.flatten(),\n    'D': D.flatten()\n})\ndf.set_index(['id', 't'], inplace=True)\nmodel = PanelOLS(df['y'], df[['D']], entity_effects=True, time_effects=True)\nresult = model.fit()\nprint(result)\n\n\n                          PanelOLS Estimation Summary                           \n================================================================================\nDep. Variable:                      y   R-squared:                        0.0032\nEstimator:                   PanelOLS   R-squared (Between):             -0.0250\nNo. Observations:                 900   R-squared (Within):              -0.0019\nDate:                Wed, Nov 22 2023   R-squared (Overall):             -0.0038\nTime:                        10:11:04   Log-likelihood                   -1218.1\nCov. Estimator:            Unadjusted                                           \n                                        F-statistic:                      2.5314\nEntities:                         100   P-value                           0.1120\nAvg Obs:                       9.0000   Distribution:                   F(1,791)\nMin Obs:                       9.0000                                           \nMax Obs:                       9.0000   F-statistic (robust):             2.5314\n                                        P-value                           0.1120\nTime periods:                       9   Distribution:                   F(1,791)\nAvg Obs:                      100.000                                           \nMin Obs:                      100.000                                           \nMax Obs:                      100.000                                           \n                                                                                \n                             Parameter Estimates                              \n==============================================================================\n            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n------------------------------------------------------------------------------\nD             -0.2136     0.1343    -1.5910     0.1120     -0.4772      0.0499\n==============================================================================\n\nF-test for Poolability: 0.7553\nP-value: 0.9657\nDistribution: F(107,791)\n\nIncluded effects: Entity, Time"
  },
  {
    "objectID": "moredid.html#weights-with-single-treatment-time",
    "href": "moredid.html#weights-with-single-treatment-time",
    "title": "Difference in Differences II",
    "section": "Weights with Single Treatment Time",
    "text": "Weights with Single Treatment Time\n\n\nCode\ndef plotD(D,ax):\n    n, T = D.shape\n    ax.set(xlabel='time',ylabel='portiontreated')\n    ax.plot(range(1,T+1),D.mean(axis=0))\n    ax\n\ndef plotweights(D, ax):\n    n, T = D.shape\n    ω = weights(D)\n    groups = np.unique(D, axis=0)\n    ax.set(xlabel='time', ylabel='weight')\n\n    for g in groups:\n        i = np.where(np.all(D == g, axis=1))[0][0]\n        wt = ω[i, :]\n        ax.plot(range(1, T+1), wt, marker='o', label=f'Treated {np.sum(g)} times')\n\n    ax.legend()\n    ax\n\ndef plotwd(D):\n    fig, ax = plt.subplots(2,1)\n    ax[0]=plotD(D,ax[0])\n    ax[1]=plotweights(D,ax[1])\n    plt.show()\n\nplotwd(D)"
  },
  {
    "objectID": "moredid.html#weights-with-early-and-late-treated",
    "href": "moredid.html#weights-with-early-and-late-treated",
    "title": "Difference in Differences II",
    "section": "Weights with Early and Late Treated",
    "text": "Weights with Early and Late Treated\n\n\nCode\npt = np.zeros(T)\npt[1] = 0.3\npt[T-2] = 0.6\nD = assigntreat(n,T,pt)\nplotwd(D)"
  },
  {
    "objectID": "moredid.html#sign-reversal",
    "href": "moredid.html#sign-reversal",
    "title": "Difference in Differences II",
    "section": "Sign Reversal",
    "text": "Sign Reversal\n\n\nCode\ndvals = np.unique(D,axis=0)\ndvals.sort()\nATT = np.ones(T)\nATT[0] = 0.0\nATT[T-2:T] = 6.0\n\ndef simulate(n,T,pt,ATT,sigma=0.01):\n    D = assigntreat(n,T,pt)\n    y = np.random.randn(n,T)*sigma + ATT[np.cumsum(D, axis=1)]\n    df = pd.DataFrame({\n        'id': np.repeat(np.arange(1, n + 1), T),\n        't': np.tile(np.arange(1, T + 1), n),\n        'y': y.flatten(),\n        'D': D.flatten()\n    })\n    df.set_index(['id', 't'], inplace=True)\n    return(df)\n\ndf = simulate(n,T,pt,ATT)\nmodel = PanelOLS(df['y'], df[['D']], entity_effects=True, time_effects=True)\nresult = model.fit()\nprint(result)\n\n\n                          PanelOLS Estimation Summary                           \n================================================================================\nDep. Variable:                      y   R-squared:                        0.0301\nEstimator:                   PanelOLS   R-squared (Between):             -0.6710\nNo. Observations:                 900   R-squared (Within):              -0.1571\nDate:                Wed, Nov 22 2023   R-squared (Overall):             -0.3896\nTime:                        10:11:05   Log-likelihood                   -1150.1\nCov. Estimator:            Unadjusted                                           \n                                        F-statistic:                      24.548\nEntities:                         100   P-value                           0.0000\nAvg Obs:                       9.0000   Distribution:                   F(1,791)\nMin Obs:                       9.0000                                           \nMax Obs:                       9.0000   F-statistic (robust):             24.548\n                                        P-value                           0.0000\nTime periods:                       9   Distribution:                   F(1,791)\nAvg Obs:                      100.000                                           \nMin Obs:                      100.000                                           \nMax Obs:                      100.000                                           \n                                                                                \n                             Parameter Estimates                              \n==============================================================================\n            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n------------------------------------------------------------------------------\nD             -0.6416     0.1295    -4.9546     0.0000     -0.8958     -0.3874\n==============================================================================\n\nF-test for Poolability: 5.9944\nP-value: 0.0000\nDistribution: F(107,791)\n\nIncluded effects: Entity, Time"
  },
  {
    "objectID": "moredid.html#when-to-worry",
    "href": "moredid.html#when-to-worry",
    "title": "Difference in Differences II",
    "section": "When to worry",
    "text": "When to worry\n\nIf multiple treatment times and treatment heterogeneity\nEven if weights do not have wrong sign, the fixed effects estimate is hard to interpret\nSame logic applies more generally – not just to time\n\nE.g. if have group effects, some treated units in multiple groups, and \\(E[y(1) - y(0) | group]\\) varies"
  },
  {
    "objectID": "moredid.html#what-to-do",
    "href": "moredid.html#what-to-do",
    "title": "Difference in Differences II",
    "section": "What to Do?",
    "text": "What to Do?\n\nFollow identification \\[\n\\begin{align*}\nATT_{t,t-s}& = \\Er[y_{it} - y_{it-s} | D_{it}=1, D_{it-s}=0] - \\Er[y_{it} - y_{it-s} | D_{it}=0, D_{it-s}=0]\n\\end{align*}\n\\] and estimate \\[\n\\begin{align*}\n\\widehat{ATT}_{t,t-s} = & \\frac{\\sum_i y_{it} D_{it}(1-D_{it-s})}{\\sum_i D_{it}(1-D_{it-s})} \\\\\n& - \\frac{\\sum_i y_{it} (1-D_{it})(1-D_{it-s})}{\\sum_i (1-D_{it})(1-D_{it-s})}\n\\end{align*}\n\\] and perhaps some average, e.g. (there are other reasonable weighted averages) \\[\n\\sum_{t=1}^T \\frac{\\sum_i D_{it}}{\\sum_{i,s} D_{i,s}} \\frac{1}{t-1} \\sum_{s=1}^{t-1} \\widehat{ATT}_{t,t-s}\n\\]\n\nCode? Inference? Optimal? (could create it, but there’s an easier way)"
  },
  {
    "objectID": "moredid.html#what-to-do-1",
    "href": "moredid.html#what-to-do-1",
    "title": "Difference in Differences II",
    "section": "What to Do?",
    "text": "What to Do?\n\nUse an appropriate package\n\ndifferences\nsee https://asjadnaqvi.github.io/DiD/ for more options (but none are python)\n\nProblem is possible correlation of \\((y_{it}(1) - y_{it}(0))D_{it}\\) with \\(\\tilde{D}_{it}\\)\n\n\\(\\tilde{D}_{it}\\) is function of \\(t\\) and \\((D_{i1}, ..., D_{iT})\\)\nEstimating separate coefficient for each combination of \\(t\\) and \\((D_{i1}, ..., D_{iT})\\) will eliminate correlation / flexibly model treatment effect heterogeneity"
  },
  {
    "objectID": "moredid.html#what-to-do-2",
    "href": "moredid.html#what-to-do-2",
    "title": "Difference in Differences II",
    "section": "What to Do?",
    "text": "What to Do?\n\nCohorts = unique sequences of \\((D_{i1}, ..., D_{iT})\\)\n\nIn current simulated example, three cohorts\n\n\\((0, 0, 0, 0, 0, 0, 0, 0, 0)\\)\n\\((0, 0, 0, 0, 0, 0, 0, 1, 1)\\)\n\\((0, 1, 1, 1, 1, 1, 1, 1, 1)\\)"
  },
  {
    "objectID": "moredid.html#regression-with-cohort-interactions",
    "href": "moredid.html#regression-with-cohort-interactions",
    "title": "Difference in Differences II",
    "section": "Regression with Cohort Interactions",
    "text": "Regression with Cohort Interactions\n\ndef definecohort(df):\n    # convert dummies into categorical\n    n = len(df.index.levels[0])\n    T = len(df.index.levels[1])\n    dmat=np.array(df.sort_index().D)\n    dmat=np.array(df.D).reshape(n,T)\n    cohort=dmat.dot(1 &lt;&lt; np.arange(dmat.shape[-1] - 1, -1, -1))\n    cdf = pd.DataFrame({\"id\":np.array(df.index.levels[0]), \"cohort\":pd.Categorical(cohort)})\n    cdf.set_index(['id'],inplace=True)\n    df=pd.merge(df, cdf, left_index=True, right_index=True)\n    return(df)\n\ndf = definecohort(df)\n\ndef defineinteractions(df):\n    df = df.reset_index()\n    df['dct'] = 'untreated'\n    df['dct'] = df.apply(lambda x: f\"t{x['t']},c{x['cohort']}\" if x['D'] else f\"untreated\", axis=1)\n    return(df.set_index(['id','t']))\n\ndf = defineinteractions(df)\n\nPanelOLS.from_formula(\"y ~ -1 + dct + EntityEffects + TimeEffects\", df).fit()\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.9999\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n1.0000\n\n\nNo. Observations:\n900\nR-squared (Within):\n0.9999\n\n\nDate:\nWed, Nov 22 2023\nR-squared (Overall):\n1.0000\n\n\nTime:\n09:38:30\nLog-likelihood\n2946.7\n\n\nCov. Estimator:\nUnadjusted\n\n\n\n\n\n\nF-statistic:\n7.047e+05\n\n\nEntities:\n100\nP-value\n0.0000\n\n\nAvg Obs:\n9.0000\nDistribution:\nF(10,782)\n\n\nMin Obs:\n9.0000\n\n\n\n\nMax Obs:\n9.0000\nF-statistic (robust):\n1.081e+06\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n9\nDistribution:\nF(10,782)\n\n\nAvg Obs:\n100.000\n\n\n\n\nMin Obs:\n100.000\n\n\n\n\nMax Obs:\n100.000\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\ndct[T.t2,c255]\n1.0026\n0.0028\n362.81\n0.0000\n0.9971\n1.0080\n\n\ndct[T.t3,c255]\n1.0028\n0.0028\n362.91\n0.0000\n0.9974\n1.0083\n\n\ndct[T.t4,c255]\n0.9988\n0.0028\n361.45\n0.0000\n0.9934\n1.0042\n\n\ndct[T.t5,c255]\n1.0001\n0.0028\n361.93\n0.0000\n0.9947\n1.0055\n\n\ndct[T.t6,c255]\n1.0015\n0.0028\n362.42\n0.0000\n0.9961\n1.0069\n\n\ndct[T.t7,c255]\n0.9988\n0.0028\n361.46\n0.0000\n0.9934\n1.0043\n\n\ndct[T.t8,c255]\n6.0006\n0.0030\n1971.9\n0.0000\n5.9947\n6.0066\n\n\ndct[T.t8,c3]\n0.9962\n0.0024\n413.34\n0.0000\n0.9915\n1.0010\n\n\ndct[T.t9,c255]\n5.9996\n0.0030\n1971.5\n0.0000\n5.9936\n6.0056\n\n\ndct[T.t9,c3]\n0.9988\n0.0024\n414.40\n0.0000\n0.9940\n1.0035\n\n\ndct[T.untreated]\n-0.0001\n0.0007\n-0.1491\n0.8815\n-0.0015\n0.0013\n\n\n\nF-test for Poolability: 0.9022P-value: 0.7452Distribution: F(107,782)Included effects: Entity, Timeid: 0x7fd24f4af910"
  },
  {
    "objectID": "moredid.html#regression-with-cohort-interactions-1",
    "href": "moredid.html#regression-with-cohort-interactions-1",
    "title": "Difference in Differences II",
    "section": "Regression with Cohort Interactions",
    "text": "Regression with Cohort Interactions\n\nIf just want to assume parallel trends at treatment times, instead of parallel trends everywhere, can estimate \\[\ny_{it} = \\sum_{c=1}^C 1\\{C_i=c\\} \\delta_{c,t} + \\alpha_i + \\epsilon_{it}\n\\]\n\\(\\hat{\\delta}_{c,t} + \\frac{\\sum \\alpha_i 1\\{C_i=c\\}}{\\sum 1\\{C_i = c\\}}\\) consistently estimates \\(\\Er[y_{it} | C_{i} = c]\\)\n\\(\\hat{\\delta}_{c,t} -\\hat{\\delta}_{c,t-s}\\) consistently estimates \\(\\Er[y_{it} - y_{i,t-s}| C_{i} = c]\\)\nIf \\(c\\) treated at \\(t\\), not at \\(t-s\\), and \\(c'\\) not treated at either and assume parallel trends, \\[\n\\hat{\\delta}_{c,t} -\\hat{\\delta}_{c,t-s} - (\\hat{\\delta}_{c',t} -\\hat{\\delta}_{c',t-s}) \\inprob \\Er[y_{it}(1)-y{it}(0)| C_i =c]\n\\]"
  },
  {
    "objectID": "moredid.html#pre-trends-1",
    "href": "moredid.html#pre-trends-1",
    "title": "Difference in Differences II",
    "section": "Pre-trends",
    "text": "Pre-trends\n\nParallel trends assumption\n\n\\[\n\\Er[\\color{red}{y_{it}(0)} - y_{it-s}(0) | D_{it}=1,  D_{it-s}=0] = \\Er[y_{it}(0) - y_{it-s}(0) | D_{it}=0, D_{it-s}=0]\n\\]\n\nMore plausible if there are parallel pre-trends\n\n\\[\n\\begin{align*}\n& \\Er[y_{it-r}(0) - y_{it-s}(0) | D_{it}=1, D_{it-r}=0,  D_{it-s}=0] = \\\\\n& = \\Er[y_{it-r}(0) - y_{it-s}(0) | D_{it}=0, D_{it-r}=0, D_{it-s}=0]\n\\end{align*}\n\\]\n\nAlways at least plot pre-trends"
  },
  {
    "objectID": "moredid.html#testing-for-pre-trends",
    "href": "moredid.html#testing-for-pre-trends",
    "title": "Difference in Differences II",
    "section": "Testing for Pre-trends",
    "text": "Testing for Pre-trends\n\nIs it a good idea to test\n\n\\[\n\\begin{align*}\nH_0 : & \\Er[y_{it-r} - y_{it-s} | D_{it}=1, D_{it-r}=0,  D_{it-s}=0] = \\\\\n& = \\Er[y_{it-r} - y_{it-s} | D_{it}=0, D_{it-r}=0, D_{it-s}=0]?\n\\end{align*}\n\\] - Even if not testing formally, we do it informally by plotting"
  },
  {
    "objectID": "moredid.html#testing-for-pre-trends-1",
    "href": "moredid.html#testing-for-pre-trends-1",
    "title": "Difference in Differences II",
    "section": "Testing for Pre-trends",
    "text": "Testing for Pre-trends\n\nDistribution of \\(\\hat{ATT}\\) conditional on fail to reject parallel pre-trends is not normal\nRoth (2022) : test can have low power, and in plausible violations, \\(\\widehat{ATT}_{3,2}\\) conditional on failing to reject is biased"
  },
  {
    "objectID": "moredid.html#bounds-from-pre-trends",
    "href": "moredid.html#bounds-from-pre-trends",
    "title": "Difference in Differences II",
    "section": "Bounds from Pre-trends",
    "text": "Bounds from Pre-trends\n\nLet \\(\\Delta\\) be violation of parallel trends \\[\n\\Delta = \\Er[\\color{red}{y_{it}(0)} - y_{it-1}(0) | D_{it}=1,  D_{it-1}=0] - \\Er[y_{it}(0) - y_{it-1}(0) | D_{it}=0, D_{it-1}=0]\n\\]\nAssume \\(\\Delta\\) is bounded by deviation from parallel of pre-trends \\[\n|\\Delta| \\leq M \\max_{r} \\left\\vert \\tau^{1t}_{t-r,t-r-1} - \\tau^{0t}_{t-r,t-r-1} \\right\\vert\n\\] for some chosen \\(M\\)\nSee Rambachan and Roth (2023)"
  },
  {
    "objectID": "moredid.html#doubly-robust-difference-in-differences",
    "href": "moredid.html#doubly-robust-difference-in-differences",
    "title": "Difference in Differences II",
    "section": "Doubly Robust Difference in Differences",
    "text": "Doubly Robust Difference in Differences\n\nLinear covariates could lead to same problem as with matching\nDoubly robust estimator Sant’Anna and Zhao (2020)\n\ndoubleml package implements it"
  },
  {
    "objectID": "moredid.html#sources-and-further-reading",
    "href": "moredid.html#sources-and-further-reading",
    "title": "Difference in Differences II",
    "section": "Sources and Further Reading",
    "text": "Sources and Further Reading\n\nFacure (2022, chap. 1)\nHuntington-Klein (2021, chap. 16)\nRecent reviews: Roth et al. (2023), Chaisemartin and D’Haultfœuille (2022)\nEarly work pointing to problems with fixed effects:\n\nLaporte and Windmeijer (2005), Wooldridge (2005)\n\nExplosion of papers written just before 2020, published just after:\n\nBorusyak and Jaravel (2018)\nChaisemartin and D’Haultfœuille (2020)\nCallaway and Sant’Anna (2021)\nGoodman-Bacon (2021)\nSun and Abraham (2021)"
  },
  {
    "objectID": "moredid.html#references",
    "href": "moredid.html#references",
    "title": "Difference in Differences II",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBorusyak, Kirill, and Xavier Jaravel. 2018. “Revisiting Event Study Designs.” https://scholar.harvard.edu/files/borusyak/files/borusyak_jaravel_event_studies.pdf.\n\n\nCallaway, Brantly, and Pedro H. C. Sant’Anna. 2021. “Difference-in-Differences with Multiple Time Periods.” Journal of Econometrics 225 (2): 200–230. https://doi.org/https://doi.org/10.1016/j.jeconom.2020.12.001.\n\n\nChaisemartin, Clément de, and Xavier D’Haultfœuille. 2020. “Two-Way Fixed Effects Estimators with Heterogeneous Treatment Effects.” American Economic Review 110 (9): 2964–96. https://doi.org/10.1257/aer.20181169.\n\n\n———. 2022. “Two-way fixed effects and differences-in-differences with heterogeneous treatment effects: a survey.” The Econometrics Journal 26 (3): C1–30. https://doi.org/10.1093/ectj/utac017.\n\n\nFacure, Matheus. 2022. Causal Inference for the Brave and True. https://matheusfacure.github.io/python-causality-handbook/landing-page.html.\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with Variation in Treatment Timing.” Journal of Econometrics 225 (2): 254–77. https://doi.org/https://doi.org/10.1016/j.jeconom.2021.03.014.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. CRC Press. https://theeffectbook.net/.\n\n\nLaporte, Audrey, and Frank Windmeijer. 2005. “Estimation of Panel Data Models with Binary Indicators When Treatment Effects Are Not Constant over Time.” Economics Letters 88 (3): 389–96. https://doi.org/https://doi.org/10.1016/j.econlet.2005.04.002.\n\n\nRambachan, Ashesh, and Jonathan Roth. 2023. “A More Credible Approach to Parallel Trends.” The Review of Economic Studies 90 (5): 2555–91. https://doi.org/10.1093/restud/rdad018.\n\n\nRoth, Jonathan. 2022. “Pretest with Caution: Event-Study Estimates After Testing for Parallel Trends.” American Economic Review: Insights 4 (3): 305–22. https://doi.org/10.1257/aeri.20210236.\n\n\nRoth, Jonathan, Pedro H. C. Sant’Anna, Alyssa Bilinski, and John Poe. 2023. “What’s Trending in Difference-in-Differences? A Synthesis of the Recent Econometrics Literature.” Journal of Econometrics 235 (2): 2218–44. https://doi.org/https://doi.org/10.1016/j.jeconom.2023.03.008.\n\n\nSant’Anna, Pedro H. C., and Jun Zhao. 2020. “Doubly Robust Difference-in-Differences Estimators.” Journal of Econometrics 219 (1): 101–22. https://doi.org/https://doi.org/10.1016/j.jeconom.2020.06.003.\n\n\nSun, Liyang, and Sarah Abraham. 2021. “Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous Treatment Effects.” Journal of Econometrics 225 (2): 175–99. https://doi.org/https://doi.org/10.1016/j.jeconom.2020.09.006.\n\n\nWooldridge, Jeffrey M. 2005. “Fixed-Effects and Related Estimators for Correlated Random-Coefficient and Treatment-Effect Panel Data Models.” The Review of Economics and Statistics 87 (2): 385–90. https://doi.org/10.1162/0034653053970320."
  },
  {
    "objectID": "moredid.html#regression-with-cohort-time-interactions",
    "href": "moredid.html#regression-with-cohort-time-interactions",
    "title": "Difference in Differences II",
    "section": "Regression with Cohort-time Interactions",
    "text": "Regression with Cohort-time Interactions\n\nEstimate: \\[\ny_{it} = \\sum_{c=1}^C D_{it} 1\\{C_i=c\\} \\beta_{ct} + \\alpha_i + \\delta_t + \\epsilon_{it}\n\\]\n\\(\\hat{\\beta}_{ct}\\) consistently estimates \\(\\Er[y_{it}(1) - y_{it}(0) | C_{i}=c, D_{it}=1]\\) is parallel trends holds for all periods \\[\n\\Er[y_{it}(0) - y_{it-s}(0) | C_i=c] = \\Er[y_{it}(0) - y_{it-s}(0) | C_i=c']\n\\] for all \\(t, s, c, c'\\)"
  },
  {
    "objectID": "moredid.html#regression-with-cohort-treat-time-interactions",
    "href": "moredid.html#regression-with-cohort-treat-time-interactions",
    "title": "Difference in Differences II",
    "section": "Regression with Cohort-Treat-Time Interactions",
    "text": "Regression with Cohort-Treat-Time Interactions\n\n\nCode\ndef definecohort(df):\n    # convert dummies into categorical\n    n = len(df.index.levels[0])\n    T = len(df.index.levels[1])\n    dmat=np.array(df.sort_index().D)\n    dmat=np.array(df.D).reshape(n,T)\n    cohort=dmat.dot(1 &lt;&lt; np.arange(dmat.shape[-1] - 1, -1, -1))\n    cdf = pd.DataFrame({\"id\":np.array(df.index.levels[0]), \"cohort\":pd.Categorical(cohort)})\n    cdf.set_index(['id'],inplace=True)\n    df=pd.merge(df, cdf, left_index=True, right_index=True)\n    return(df)\n\ndf = definecohort(df)\n\ndef defineinteractions(df):\n    df = df.reset_index()\n    df['dct'] = 'untreated'\n    df['dct'] = df.apply(lambda x: f\"t{x['t']},c{x['cohort']}\" if x['D'] else f\"untreated\", axis=1)\n    return(df.set_index(['id','t']))\n\ndf = defineinteractions(df)\n\nPanelOLS.from_formula(\"y ~ -1 + dct + EntityEffects + TimeEffects\", df).fit()\n\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.9999\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n1.0000\n\n\nNo. Observations:\n900\nR-squared (Within):\n0.9999\n\n\nDate:\nWed, Nov 22 2023\nR-squared (Overall):\n1.0000\n\n\nTime:\n10:11:05\nLog-likelihood\n2904.1\n\n\nCov. Estimator:\nUnadjusted\n\n\n\n\n\n\nF-statistic:\n6.594e+05\n\n\nEntities:\n100\nP-value\n0.0000\n\n\nAvg Obs:\n9.0000\nDistribution:\nF(10,782)\n\n\nMin Obs:\n9.0000\n\n\n\n\nMax Obs:\n9.0000\nF-statistic (robust):\n1.077e+06\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n9\nDistribution:\nF(10,782)\n\n\nAvg Obs:\n100.000\n\n\n\n\nMin Obs:\n100.000\n\n\n\n\nMax Obs:\n100.000\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\ndct[T.t2,c255]\n0.9976\n0.0028\n359.77\n0.0000\n0.9922\n1.0030\n\n\ndct[T.t3,c255]\n0.9997\n0.0028\n360.52\n0.0000\n0.9942\n1.0051\n\n\ndct[T.t4,c255]\n0.9996\n0.0028\n360.50\n0.0000\n0.9942\n1.0050\n\n\ndct[T.t5,c255]\n0.9980\n0.0028\n359.90\n0.0000\n0.9925\n1.0034\n\n\ndct[T.t6,c255]\n0.9944\n0.0028\n358.63\n0.0000\n0.9890\n0.9999\n\n\ndct[T.t7,c255]\n0.9980\n0.0028\n359.91\n0.0000\n0.9925\n1.0034\n\n\ndct[T.t8,c255]\n5.9981\n0.0032\n1880.0\n0.0000\n5.9919\n6.0044\n\n\ndct[T.t8,c3]\n1.0012\n0.0027\n377.34\n0.0000\n0.9960\n1.0064\n\n\ndct[T.t9,c255]\n5.9996\n0.0032\n1880.5\n0.0000\n5.9933\n6.0058\n\n\ndct[T.t9,c3]\n1.0029\n0.0027\n377.98\n0.0000\n0.9977\n1.0081\n\n\ndct[T.untreated]\n0.0006\n0.0008\n0.7037\n0.4819\n-0.0010\n0.0021\n\n\n\nF-test for Poolability: 0.9401P-value: 0.6493Distribution: F(107,782)Included effects: Entity, Timeid: 0x7f44e4de6e90"
  },
  {
    "objectID": "moredid.html#regression-with-cohort-time-interactions-1",
    "href": "moredid.html#regression-with-cohort-time-interactions-1",
    "title": "Difference in Differences II",
    "section": "Regression with Cohort-Time Interactions",
    "text": "Regression with Cohort-Time Interactions\n\nIf just want to assume parallel trends at treatment times, instead of parallel trends everywhere, can estimate \\[\ny_{it} = \\sum_{c=1}^C 1\\{C_i=c\\} \\delta_{c,t} + \\alpha_i + \\epsilon_{it}\n\\]\n\\(\\hat{\\delta}_{c,t} + \\frac{\\sum \\alpha_i 1\\{C_i=c\\}}{\\sum 1\\{C_i = c\\}}\\) consistently estimates \\(\\Er[y_{it} | C_{i} = c]\\)\n\\(\\hat{\\delta}_{c,t} -\\hat{\\delta}_{c,t-s}\\) consistently estimates \\(\\Er[y_{it} - y_{i,t-s}| C_{i} = c]\\)\nIf \\(c\\) treated at \\(t\\), not at \\(t-s\\), and \\(c'\\) not treated at either and assume parallel trends, \\[\n\\hat{\\delta}_{c,t} -\\hat{\\delta}_{c,t-s} - (\\hat{\\delta}_{c',t} -\\hat{\\delta}_{c',t-s}) \\inprob \\Er[y_{it}(1)-y{it}(0)| C_i =c]\n\\]"
  },
  {
    "objectID": "moredid.html#regression-with-cohort-time-interactions-2",
    "href": "moredid.html#regression-with-cohort-time-interactions-2",
    "title": "Difference in Differences II",
    "section": "Regression with Cohort-Time Interactions",
    "text": "Regression with Cohort-Time Interactions\n\ndfi=df.reset_index()\ndfi['time'] = dfi['t']\ndfi=dfi.set_index(['id','t'])\nPanelOLS.from_formula(\"y ~ -1 + C(cohort)*C(time) + EntityEffects\",dfi, drop_absorbed=True).fit()\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.9999\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n1.0000\n\n\nNo. Observations:\n900\nR-squared (Within):\n0.9999\n\n\nDate:\nWed, Nov 22 2023\nR-squared (Overall):\n1.0000\n\n\nTime:\n10:11:05\nLog-likelihood\n2905.8\n\n\nCov. Estimator:\nUnadjusted\n\n\n\n\n\n\nF-statistic:\n5.209e+05\n\n\nEntities:\n100\nP-value\n0.0000\n\n\nAvg Obs:\n9.0000\nDistribution:\nF(24,776)\n\n\nMin Obs:\n9.0000\n\n\n\n\nMax Obs:\n9.0000\nF-statistic (robust):\n6.934e+05\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n9\nDistribution:\nF(24,776)\n\n\nAvg Obs:\n100.000\n\n\n\n\nMin Obs:\n100.000\n\n\n\n\nMax Obs:\n100.000\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nC(cohort)[T.0]\n0.0028\n0.0043\n0.6503\n0.5157\n-0.0056\n0.0112\n\n\nC(time)[T.2]\n-0.0030\n0.0030\n-0.9999\n0.3177\n-0.0088\n0.0029\n\n\nC(time)[T.3]\n-0.0030\n0.0030\n-1.0100\n0.3128\n-0.0089\n0.0028\n\n\nC(time)[T.4]\n-0.0011\n0.0030\n-0.3631\n0.7166\n-0.0069\n0.0048\n\n\nC(time)[T.5]\n3.129e-05\n0.0030\n0.0105\n0.9916\n-0.0058\n0.0059\n\n\nC(time)[T.6]\n-0.0030\n0.0030\n-1.0033\n0.3160\n-0.0088\n0.0029\n\n\nC(time)[T.7]\n-0.0020\n0.0030\n-0.6801\n0.4966\n-0.0079\n0.0038\n\n\nC(time)[T.8]\n-0.0005\n0.0030\n-0.1739\n0.8620\n-0.0064\n0.0053\n\n\nC(time)[T.9]\n-0.0028\n0.0030\n-0.9534\n0.3407\n-0.0087\n0.0030\n\n\nC(cohort)[T.3]:C(time)[T.2]\n0.0043\n0.0037\n1.1755\n0.2401\n-0.0029\n0.0115\n\n\nC(cohort)[T.255]:C(time)[T.2]\n0.9999\n0.0040\n250.12\n0.0000\n0.9920\n1.0077\n\n\nC(cohort)[T.3]:C(time)[T.3]\n0.0028\n0.0037\n0.7644\n0.4448\n-0.0044\n0.0100\n\n\nC(cohort)[T.255]:C(time)[T.3]\n1.0010\n0.0040\n250.39\n0.0000\n0.9931\n1.0088\n\n\nC(cohort)[T.3]:C(time)[T.4]\n0.0041\n0.0037\n1.1102\n0.2672\n-0.0031\n0.0113\n\n\nC(cohort)[T.255]:C(time)[T.4]\n1.0017\n0.0040\n250.59\n0.0000\n0.9939\n1.0096\n\n\nC(cohort)[T.3]:C(time)[T.5]\n0.0010\n0.0037\n0.2846\n0.7760\n-0.0062\n0.0083\n\n\nC(cohort)[T.255]:C(time)[T.5]\n0.9981\n0.0040\n249.68\n0.0000\n0.9902\n1.0059\n\n\nC(cohort)[T.3]:C(time)[T.6]\n0.0040\n0.0037\n1.0869\n0.2774\n-0.0032\n0.0112\n\n\nC(cohort)[T.255]:C(time)[T.6]\n0.9965\n0.0040\n249.28\n0.0000\n0.9886\n1.0043\n\n\nC(cohort)[T.3]:C(time)[T.7]\n0.0007\n0.0037\n0.1949\n0.8455\n-0.0065\n0.0079\n\n\nC(cohort)[T.255]:C(time)[T.7]\n0.9979\n0.0040\n249.62\n0.0000\n0.9900\n1.0057\n\n\nC(cohort)[T.3]:C(time)[T.8]\n1.0031\n0.0037\n272.91\n0.0000\n0.9959\n1.0103\n\n\nC(cohort)[T.255]:C(time)[T.8]\n5.9992\n0.0040\n1500.7\n0.0000\n5.9913\n6.0070\n\n\nC(cohort)[T.3]:C(time)[T.9]\n1.0048\n0.0037\n273.37\n0.0000\n0.9976\n1.0120\n\n\nC(cohort)[T.255]:C(time)[T.9]\n6.0006\n0.0040\n1501.1\n0.0000\n5.9928\n6.0085\n\n\n\nF-test for Poolability: 0.9390P-value: 0.6458Distribution: F(99,776)Included effects: Entityid: 0x7f44e40e9010"
  },
  {
    "objectID": "syntheticcontrol.html#setup",
    "href": "syntheticcontrol.html#setup",
    "title": "Synthetic Control",
    "section": "Setup",
    "text": "Setup\n\n1 treated unit, observed \\(T_0\\) periods before treatment, \\(T_1\\) periods after\n\\(J\\) untreated units\n\\(J\\), \\(T_0\\) moderate in size\nFormalisation of comparative case study"
  },
  {
    "objectID": "syntheticcontrol.html#example-california-tobacco-control-program",
    "href": "syntheticcontrol.html#example-california-tobacco-control-program",
    "title": "Synthetic Control",
    "section": "Example: California Tobacco Control Program",
    "text": "Example: California Tobacco Control Program\n\nCode and copy of data from Facure (2022)\nData used in Abadie, Diamond, and Hainmueller (2010)\n\n\n\nCode\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import style\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as smf\n\n#if 'ipykernel' in sys.modules:\n#    %matplotlib inline\n\npd.set_option(\"display.max_columns\", 20)\nstyle.use(\"fivethirtyeight\")"
  },
  {
    "objectID": "syntheticcontrol.html#data-california-tobacco-control-program",
    "href": "syntheticcontrol.html#data-california-tobacco-control-program",
    "title": "Synthetic Control",
    "section": "Data: California Tobacco Control Program",
    "text": "Data: California Tobacco Control Program\n\ncigar = pd.read_csv(\"data/smoking.csv\")\ncigar.query(\"california\").head()\n\n\n\n\n\n\n\n\nstate\nyear\ncigsale\nlnincome\nbeer\nage15to24\nretprice\ncalifornia\nafter_treatment\n\n\n\n\n62\n3\n1970\n123.000000\nNaN\nNaN\n0.178158\n38.799999\nTrue\nFalse\n\n\n63\n3\n1971\n121.000000\nNaN\nNaN\n0.179296\n39.700001\nTrue\nFalse\n\n\n64\n3\n1972\n123.500000\n9.930814\nNaN\n0.180434\n39.900002\nTrue\nFalse\n\n\n65\n3\n1973\n124.400002\n9.955092\nNaN\n0.181572\n39.900002\nTrue\nFalse\n\n\n66\n3\n1974\n126.699997\n9.947999\nNaN\n0.182710\n41.900002\nTrue\nFalse"
  },
  {
    "objectID": "syntheticcontrol.html#cigarette-sales-trends",
    "href": "syntheticcontrol.html#cigarette-sales-trends",
    "title": "Synthetic Control",
    "section": "Cigarette Sales Trends",
    "text": "Cigarette Sales Trends\n\nax = plt.subplot(1, 1, 1)\nfor gdf in cigar.groupby(\"state\"):\n    ax.plot(gdf[1]['year'],gdf[1]['cigsale'], alpha=0.2, lw=1, color=\"k\")\n\nax.set_ylim(40, 150)\nax.plot(cigar.query(\"california\")['year'], cigar.query(\"california\")['cigsale'], label=\"California\")\ncigar.query(\"not california\").groupby(\"year\")['cigsale'].mean().plot(ax=ax, label=\"Mean Other States\")\n\nplt.vlines(x=1988, ymin=40, ymax=140, linestyle=\":\", lw=2, label=\"Proposition 99\")\nplt.ylabel(\"Per-capita cigarette sales (in packs)\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "syntheticcontrol.html#which-comparison",
    "href": "syntheticcontrol.html#which-comparison",
    "title": "Synthetic Control",
    "section": "Which Comparison?",
    "text": "Which Comparison?\n\n\nAll states?\nStates bordering California?\nStates with similar characteristics? Which characteristics?"
  },
  {
    "objectID": "syntheticcontrol.html#synthetic-control",
    "href": "syntheticcontrol.html#synthetic-control",
    "title": "Synthetic Control",
    "section": "Synthetic Control",
    "text": "Synthetic Control\n\nCompare treated unit (California) to weighted average of untreated units\nWeights chosen to make sythetic control close to California"
  },
  {
    "objectID": "syntheticcontrol.html#synthetic-control-1",
    "href": "syntheticcontrol.html#synthetic-control-1",
    "title": "Synthetic Control",
    "section": "Synthetic Control",
    "text": "Synthetic Control\n\nPotential outcomes \\(Y_{it}(0), Y_{it}(1)\\)\nFor \\(t&gt;T_0\\), estimate treatment effect on treated unit as \\[\n\\hat{\\tau}_{1t} = Y_{1t} -\n\\underbrace{\\sum_{j=2}^{J+1} \\hat{w}_j Y_{jt}}_{\\hat{Y}_{1t}(0)}\n\\]\n\\(\\hat{w}_j\\) chosen to make synthetic control close to treated unit in before treatment"
  },
  {
    "objectID": "syntheticcontrol.html#weights",
    "href": "syntheticcontrol.html#weights",
    "title": "Synthetic Control",
    "section": "Weights",
    "text": "Weights\n\nWeights minimize difference \\[\n\\begin{align*}\n\\hat{W} = & \\textrm{arg}\\min_{W \\in \\R^J} \\Vert \\mathbf{X}_1 - \\mathbf{X}_0 W \\Vert_V \\\\\n& s.t. \\sum_{j=2}^{J+1} w_j = 1 \\\\\n& \\;\\;\\; 0 \\leq w_j \\leq 1 \\;\\; \\forall j\n\\end{align*}\n\\]\n\n\n\n\\(\\Vert x \\Vert_V = x' V x\\) is a weighted norm. Choose \\(V\\) to e.g. weight by inverse variance"
  },
  {
    "objectID": "syntheticcontrol.html#computing-weights",
    "href": "syntheticcontrol.html#computing-weights",
    "title": "Synthetic Control",
    "section": "Computing Weights",
    "text": "Computing Weights\n\nfeatures = [\"cigsale\", \"retprice\"]\ninverted = (cigar.query(\"~after_treatment\") # filter pre-intervention period\n            .pivot(index='state', columns=\"year\")[features] # make one column per year and one row per state\n            .T) # flip the table to have one column per state\ninverted.head()\n\n\n\n\n\n\n\n\nstate\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n...\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncigsale\n1970\n89.800003\n100.300003\n123.000000\n124.800003\n120.000000\n155.000000\n109.900002\n102.400002\n124.800003\n134.600006\n...\n103.599998\n92.699997\n99.800003\n106.400002\n65.500000\n122.599998\n124.300003\n114.500000\n106.400002\n132.199997\n\n\n1971\n95.400002\n104.099998\n121.000000\n125.500000\n117.599998\n161.100006\n115.699997\n108.500000\n125.599998\n139.300003\n...\n115.000000\n96.699997\n106.300003\n108.900002\n67.699997\n124.400002\n128.399994\n111.500000\n105.400002\n131.699997\n\n\n1972\n101.099998\n103.900002\n123.500000\n134.300003\n110.800003\n156.300003\n117.000000\n126.099998\n126.599998\n149.199997\n...\n118.699997\n103.000000\n111.500000\n108.599998\n71.300003\n138.000000\n137.000000\n117.500000\n108.800003\n140.000000\n\n\n1973\n102.900002\n108.000000\n124.400002\n137.899994\n109.300003\n154.699997\n119.800003\n121.800003\n124.400002\n156.000000\n...\n125.500000\n103.500000\n109.699997\n110.400002\n72.699997\n146.800003\n143.100006\n116.599998\n109.500000\n141.199997\n\n\n1974\n108.199997\n109.699997\n126.699997\n132.800003\n112.400002\n151.300003\n123.699997\n125.599998\n131.899994\n159.600006\n...\n129.699997\n108.400002\n114.800003\n114.699997\n75.599998\n151.800003\n149.600006\n119.900002\n111.800003\n145.800003\n\n\n\n\n5 rows × 39 columns"
  },
  {
    "objectID": "syntheticcontrol.html#examining-weights",
    "href": "syntheticcontrol.html#examining-weights",
    "title": "Synthetic Control",
    "section": "Examining Weights",
    "text": "Examining Weights\n\nWeights tend to be sparse\nGood idea to examine which untreated units get positive weight\nShould look at state names, but the data does not have them, and the state variable is not FIPs code or any standard identifier\n\n\ncalif_weights = get_w(X0, X1)\nprint(\"Sum:\", calif_weights.sum())\nnp.round(calif_weights, 4)\n\nSum: 1.000000000000424\n\n\narray([0.    , 0.    , 0.    , 0.0852, 0.    , 0.    , 0.    , 0.    ,\n       0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n       0.    , 0.    , 0.    , 0.113 , 0.1051, 0.4566, 0.    , 0.    ,\n       0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n       0.2401, 0.    , 0.    , 0.    , 0.    , 0.    ])"
  },
  {
    "objectID": "syntheticcontrol.html#effect-of-california-tobacco-control-program",
    "href": "syntheticcontrol.html#effect-of-california-tobacco-control-program",
    "title": "Synthetic Control",
    "section": "Effect of California Tobacco Control Program",
    "text": "Effect of California Tobacco Control Program\n\ncalif_synth = cigar.query(\"~california\").pivot(index='year', columns=\"state\")[\"cigsale\"].values.dot(calif_weights)\nplt.figure(figsize=(10,6))\nplt.plot(cigar.query(\"california\")[\"year\"], cigar.query(\"california\")[\"cigsale\"], label=\"California\")\nplt.plot(cigar.query(\"california\")[\"year\"], calif_synth, label=\"Synthetic Control\")\nplt.vlines(x=1988, ymin=40, ymax=140, linestyle=\":\", lw=2, label=\"Proposition 99\")\nplt.ylabel(\"Per-capita cigarette sales (in packs)\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "syntheticcontrol.html#when-does-this-work",
    "href": "syntheticcontrol.html#when-does-this-work",
    "title": "Synthetic Control",
    "section": "When Does this Work?",
    "text": "When Does this Work?\n\nIf data generated by linear factor model: \\[\nY_{jt}(0) = \\delta_t + \\theta_t Z_j + \\lambda_t \\mu_j + \\epsilon_{jt}\n\\]\n\nObserved \\(Z_j\\)\nUnobserved\\(\\lambda_t\\), \\(\\mu_j\\)\n\nAs \\(T_0\\) increases or variance of \\(\\epsilon_{jt}\\) decreases, bias of \\(\\hat{\\tau}_{it}\\) decreases\n\nNeeds \\(\\mathbf{X}_1 \\approx \\mathbf{X}_0 W\\)"
  },
  {
    "objectID": "syntheticcontrol.html#choices",
    "href": "syntheticcontrol.html#choices",
    "title": "Synthetic Control",
    "section": "Choices",
    "text": "Choices\n\n\nVariables in \\(\\mathbf{X}\\) to match\n\nFewer make eaiser to have \\(\\mathbf{X}_1 \\approx \\mathbf{X}_0 W\\)\nBut fewer make \\(W\\) depend more on \\(\\epsilon\\)\n\nSet of untreated units to consider\n\nMore units makes eaiser to have \\(\\mathbf{X}_1 \\approx \\mathbf{X}_0 W\\)\nBut risk of overfitting"
  },
  {
    "objectID": "syntheticcontrol.html#inference-1",
    "href": "syntheticcontrol.html#inference-1",
    "title": "Synthetic Control",
    "section": "Inference",
    "text": "Inference\n\nEstimator \\[\n\\hat{\\tau}_{1t} = Y_{1t} -\n\\underbrace{\\sum_{j=2}^{J+1} \\hat{w}_j Y_{jt}}_{\\hat{Y}_{1t}(0)}\n\\]\n\nsingle observation of \\(Y_{1t}\\)\n\\(\\hat{w}_j\\) depends on pre-treatment variables \\(\\underbrace{\\mathbf{X}_1, \\mathbf{X}_0}_{(J+1) \\times (T_0 + K)}\\)\n\\(J\\) values of \\(Y_{jt}\\)\n\nUsual asymptotics not applicable"
  },
  {
    "objectID": "syntheticcontrol.html#treatment-permutation",
    "href": "syntheticcontrol.html#treatment-permutation",
    "title": "Synthetic Control",
    "section": "Treatment Permutation",
    "text": "Treatment Permutation\n\nCompute estimate pretending each of the \\(J\\) untreated units were treated instead\nUse as distribution of \\(\\hat{\\tau}_{1t}\\) under \\(H_0: \\tau_{1t} = 0\\)"
  },
  {
    "objectID": "syntheticcontrol.html#treatment-permutation-1",
    "href": "syntheticcontrol.html#treatment-permutation-1",
    "title": "Synthetic Control",
    "section": "Treatment Permutation",
    "text": "Treatment Permutation\n\ndef synthetic_control(state: int, data: pd.DataFrame) -&gt; np.array:\n    features = [\"cigsale\", \"retprice\"]\n    inverted = (data.query(\"~after_treatment\")\n                .pivot(index='state', columns=\"year\")[features]\n                .T)\n    X1 = inverted[state].values # treated\n    X0 = inverted.drop(columns=state).values # donor pool\n\n    weights = get_w(X0, X1)\n    synthetic = (data.query(f\"~(state=={state})\")\n                 .pivot(index='year', columns=\"state\")[\"cigsale\"]\n                 .values.dot(weights))\n    return (data\n            .query(f\"state=={state}\")[[\"state\", \"year\", \"cigsale\", \"after_treatment\"]]\n            .assign(synthetic=synthetic))\n\nfrom joblib import Parallel, delayed\n\ncontrol_pool = cigar[\"state\"].unique()\nparallel_fn = delayed(partial(synthetic_control, data=cigar))\nsynthetic_states = Parallel(n_jobs=20)(parallel_fn(state) for state in control_pool);"
  },
  {
    "objectID": "syntheticcontrol.html#treatment-permutation-inference",
    "href": "syntheticcontrol.html#treatment-permutation-inference",
    "title": "Synthetic Control",
    "section": "Treatment Permutation Inference",
    "text": "Treatment Permutation Inference\n\n\nCode\ndef pre_treatment_error(state):\n    pre_treat_error = (state.query(\"~after_treatment\")[\"cigsale\"]\n                       - state.query(\"~after_treatment\")[\"synthetic\"]) ** 2\n    return pre_treat_error.mean()\n\nplt.figure(figsize=(12,7))\nfor state in synthetic_states:\n\n    # remove units with mean error above 80.\n    if pre_treatment_error(state) &lt; 80:\n        plt.plot(state[\"year\"], state[\"cigsale\"] - state[\"synthetic\"], color=\"C5\",alpha=0.4)\n\nplt.plot(cigar.query(\"california\")[\"year\"], cigar.query(\"california\")[\"cigsale\"] - calif_synth,\n        label=\"California\");\n\nplt.vlines(x=1988, ymin=-50, ymax=120, linestyle=\":\", lw=2, label=\"Proposition 99\")\nplt.hlines(y=0, xmin=1970, xmax=2000, lw=3)\nplt.ylabel(\"Gap in per-capita cigarette sales (in packs)\")\nplt.title(\"Distribution of Effects\")\nplt.title(\"State - Synthetic Across Time (Large Pre-Treatment Errors Removed)\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "syntheticcontrol.html#treatment-permutation-inference-testing-h_0-tau0",
    "href": "syntheticcontrol.html#treatment-permutation-inference-testing-h_0-tau0",
    "title": "Synthetic Control",
    "section": "Treatment Permutation Inference: Testing \\(H_0: \\tau=0\\)",
    "text": "Treatment Permutation Inference: Testing \\(H_0: \\tau=0\\)\n\ncalif_number = 3\n\neffects = [state.query(\"year==2000\").iloc[0][\"cigsale\"] - state.query(\"year==2000\").iloc[0][\"synthetic\"]\n           for state in synthetic_states\n           if pre_treatment_error(state) &lt; 80] # filter out noise\n\ncalif_effect = cigar.query(\"california & year==2000\").iloc[0][\"cigsale\"] - calif_synth[-1]\n\nprint(\"California Treatment Effect for the Year 2000:\", calif_effect)\nnp.array(effects)\n\nnp.mean(np.array(effects) &lt; calif_effect)\n\nCalifornia Treatment Effect for the Year 2000: -24.83015975607075\n\n\n0.02857142857142857"
  },
  {
    "objectID": "syntheticcontrol.html#design-based-inference",
    "href": "syntheticcontrol.html#design-based-inference",
    "title": "Synthetic Control",
    "section": "Design Based Inference",
    "text": "Design Based Inference\n\n\nSampling based inference:\n\nSpecify data generating process (DGP)\nFind distribution of estimator under repeated sampling of datasets from DGP\n\n\n\n\n\nDesign based inference:\n\nCondition on dataset you have\nRandomness of estimator from random assignment of treatment\nSee Abadie et al. (2020) for more information\n\n\n\n\n\nTreatment permutation inference is design based inference assuming the treated unit was chosen uniformly at random from all units"
  },
  {
    "objectID": "syntheticcontrol.html#design-based-treatment-permutation-inference",
    "href": "syntheticcontrol.html#design-based-treatment-permutation-inference",
    "title": "Synthetic Control",
    "section": "Design Based / Treatment Permutation Inference",
    "text": "Design Based / Treatment Permutation Inference\n\nPros:\n\nEasy to implement and explain\nIntuitive appeal, similar to placebo tests\nMinimal assumptions about DGP\n\nCons:\n\nAssumption about randomized treatment assignment is generally false\nNeeds modification to test hypotheses other than \\(H_0: \\tau=0\\) & to construct confidence intervals"
  },
  {
    "objectID": "syntheticcontrol.html#prediction-intervals",
    "href": "syntheticcontrol.html#prediction-intervals",
    "title": "Synthetic Control",
    "section": "Prediction Intervals",
    "text": "Prediction Intervals\n\nEstimation error is a prediction error \\[\n\\begin{align*}\n\\hat{\\tau}_{1t} = & Y_{1t} -\n\\underbrace{\\hat{Y}_{1t}(0)}_{\\text{prediction given $Y_{jt}, \\mathbf{X}_0$}}\n\\\\\n= &  \\underbrace{Y_{1t}(1) - \\color{grey}{Y_{1t}(0)}}_{\\tau_{1t}} + \\underbrace{\\color{grey}{Y_{1t}(0)} - \\hat{Y}_{1t}(0)}_{\\text{prediction error}}\n\\end{align*}\n\\]\n\n\n\nMany statistical methods for calculating distribution of prediction error\nDifficulties:\n\nHigh-dimensional: number of weights comparable to number of observations and dimension of predictors\nModerate sample sizes"
  },
  {
    "objectID": "syntheticcontrol.html#prediction-intervals-1",
    "href": "syntheticcontrol.html#prediction-intervals-1",
    "title": "Synthetic Control",
    "section": "Prediction Intervals",
    "text": "Prediction Intervals\n\nModern approaches accomodate high-dimensionality and give non-asymptotic results\nChernozhukov, Wüthrich, and Zhu (2021) : construct prediction intervals by permuting residuals (conformal inference)\nCattaneo, Feng, and Titiunik (2021) : divide prediction error into two pieces: estimation of \\(\\hat{w}\\) and unpredictable randomness in \\(Y_{1t}(0)\\)"
  },
  {
    "objectID": "syntheticcontrol.html#catteneo-feng-titiunik",
    "href": "syntheticcontrol.html#catteneo-feng-titiunik",
    "title": "Synthetic Control",
    "section": "Catteneo, Feng, & Titiunik",
    "text": "Catteneo, Feng, & Titiunik\n\nGiven coverage level \\(\\alpha\\), gives interval \\(\\mathcal{I}_{1-\\alpha}\\) such that \\[\nP\\left[ P\\left(\\tau_{1t} \\in \\mathcal{I}_{1-\\alpha} | \\mathbf{X}_0, \\{y_{jt}\\}_{j=1}^J\\right) &gt; 1-\\alpha-\\epsilon(T_0) \\right] &gt; 1 - \\pi(T_0)\n\\] where \\(\\epsilon(T_0) \\to 0\\) and \\(\\pi(T_0) \\to 0\\) as \\(T_0 \\to \\infty\\)\nprediction error comes from\n\nestimation of \\(\\hat{w}\\), options refer to u_ in scpi_pkg\nunobservable stochastic error in \\(Y_{1t}(0)\\), options begin with e_ in scpi_pkg\n\nvalid under broad range of DGPs, but appropriate interval does depend on dependence of data over time, stationarity, assumptions about distribution of error given predictors"
  },
  {
    "objectID": "syntheticcontrol.html#software",
    "href": "syntheticcontrol.html#software",
    "title": "Synthetic Control",
    "section": "Software",
    "text": "Software\n\nscpi_pkg recommended, created by leading econometricians\npysyncon actively maintained, well-documented, but appears not popular\nSpareSC created by researchers at Microsoft, uses particular variant\nscinference R package accompanying Chernozhukov, Wüthrich, and Zhu (2021)"
  },
  {
    "objectID": "syntheticcontrol.html#data-preparation",
    "href": "syntheticcontrol.html#data-preparation",
    "title": "Synthetic Control",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nimport random\nfrom scpi_pkg.scdata import scdata\nfrom scpi_pkg.scest import scest\nfrom scpi_pkg.scplot import scplot\nscdf = scdata(df=cigar, id_var=\"state\", time_var=\"year\", outcome_var=\"cigsale\",\n              period_pre=cigar.query(\"not after_treatment\").year.unique(),\n              period_post=cigar.query(\"after_treatment\").year.unique(),\n              unit_tr=calif_number,\n              unit_co=cigar.query(\"not california\").state.unique(),\n              features=[\"cigsale\",\"retprice\"],\n              cov_adj=None, cointegrated_data=True,\n              constant=False)"
  },
  {
    "objectID": "syntheticcontrol.html#point-estimation",
    "href": "syntheticcontrol.html#point-estimation",
    "title": "Synthetic Control",
    "section": "Point Estimation",
    "text": "Point Estimation\n\n\nCode\nest_si = scest(scdf, w_constr={'name': \"simplex\"})\nprint(est_si)\nest_si2 = scest(scdf, w_constr={'p': 'L1', 'dir': '==', 'Q': 1, 'lb': 0})\nprint(est_si2)\nscplot(est_si)\n\n\n-----------------------------------------------------------------------\nCall: scest\nSynthetic Control Estimation - Setup\n\nConstraint Type:                                                simplex\nConstraint Size (Q):                                                  1\nTreated Unit:                                                         3\nSize of the donor pool:                                              38\nFeatures                                                              2\nPre-treatment period                                          1970-1988\nPre-treatment periods used in estimation per feature:\n Feature  Observations\n cigsale            19\nretprice            19\nCovariates used for adjustment per feature:\n Feature  Num of Covariates\n cigsale                  0\nretprice                  0\n\nSynthetic Control Estimation - Results\n\nActive donors: 5\n\nCoefficients:\n                    Weights\nTreated Unit Donor         \n3            1        0.000\n             10       0.000\n             11       0.000\n             12       0.000\n             13       0.000\n             14       0.000\n             15       0.000\n             16       0.000\n             17       0.000\n             18       0.000\n             19       0.000\n             2        0.000\n             20       0.000\n             21       0.113\n             22       0.105\n             23       0.457\n             24       0.000\n             25       0.000\n             26       0.000\n             27       0.000\n             28       0.000\n             29       0.000\n             30       0.000\n             31       0.000\n             32       0.000\n             33       0.000\n             34       0.240\n             35       0.000\n             36       0.000\n             37       0.000\n             38       0.000\n             39       0.000\n             4        0.000\n             5        0.085\n             6        0.000\n             7        0.000\n             8        0.000\n             9        0.000\n\n-----------------------------------------------------------------------\nCall: scest\nSynthetic Control Estimation - Setup\n\nConstraint Type:                                          user provided\nConstraint Size (Q):                                                  1\nTreated Unit:                                                         3\nSize of the donor pool:                                              38\nFeatures                                                              2\nPre-treatment period                                          1970-1988\nPre-treatment periods used in estimation per feature:\n Feature  Observations\n cigsale            19\nretprice            19\nCovariates used for adjustment per feature:\n Feature  Num of Covariates\n cigsale                  0\nretprice                  0\n\nSynthetic Control Estimation - Results\n\nActive donors: 5\n\nCoefficients:\n                    Weights\nTreated Unit Donor         \n3            1        0.000\n             10       0.000\n             11       0.000\n             12       0.000\n             13       0.000\n             14       0.000\n             15       0.000\n             16       0.000\n             17       0.000\n             18       0.000\n             19       0.000\n             2        0.000\n             20       0.000\n             21       0.113\n             22       0.105\n             23       0.457\n             24       0.000\n             25       0.000\n             26       0.000\n             27       0.000\n             28       0.000\n             29       0.000\n             30       0.000\n             31       0.000\n             32       0.000\n             33       0.000\n             34       0.240\n             35       0.000\n             36       0.000\n             37       0.000\n             38       0.000\n             39       0.000\n             4        0.000\n             5        0.085\n             6        0.000\n             7        0.000\n             8        0.000\n             9        0.000\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;"
  },
  {
    "objectID": "syntheticcontrol.html#inference-2",
    "href": "syntheticcontrol.html#inference-2",
    "title": "Synthetic Control",
    "section": "Inference",
    "text": "Inference\n\n\nCode\nfrom scpi_pkg.scpi import scpi\nimport random\nw_constr = {'name': 'simplex', 'Q': 1}\nu_missp = True\nu_sigma = \"HC1\"\nu_order = 1\nu_lags = 0\ne_method = \"gaussian\"\ne_order = 1\ne_lags = 0\ne_alpha = 0.05\nu_alpha = 0.05\nsims = 200\ncores = 1\n\nrandom.seed(8894)\nresult = scpi(scdf, sims=sims, w_constr=w_constr, u_order=u_order, u_lags=u_lags,\n              e_order=e_order, e_lags=e_lags, e_method=e_method, u_missp=u_missp,\n              u_sigma=u_sigma, cores=cores, e_alpha=e_alpha, u_alpha=u_alpha)\nscplot(result, e_out=True, x_lab=\"year\", y_lab=\"per-capita cigarette sales (in packs)\")\n\n\n-----------------------------------------------\nEstimating Weights...\nQuantifying Uncertainty\nMaximum expected execution time: 2 minutes.\n \n\n20/200 iterations completed (10%)\n40/200 iterations completed (20%)\n60/200 iterations completed (30%)\n80/200 iterations completed (40%)\n100/200 iterations completed (50%)\n120/200 iterations completed (60%)\n140/200 iterations completed (70%)\n160/200 iterations completed (80%)\n180/200 iterations completed (90%)\n200/200 iterations completed (100%)\n\n\n\n&lt;Figure Size: (640 x 480)&gt;"
  },
  {
    "objectID": "syntheticcontrol.html#sources-and-further-reading",
    "href": "syntheticcontrol.html#sources-and-further-reading",
    "title": "Synthetic Control",
    "section": "Sources and Further Reading",
    "text": "Sources and Further Reading\n\nexample code from Facure (2022) chapter 15\nnotation and theory follows Abadie (2021)\nHuntington-Klein (2021) chapter 21.2\nMore technical:\n\nCattaneo et al. (2022) user guide for scpi_pkg\nAbadie, Diamond, and Hainmueller (2010) important paper popularizing synthetic control and permutation inference\nAbadie and Cattaneo (2021) lists recent statistical advances in synthetic control"
  },
  {
    "objectID": "syntheticcontrol.html#references",
    "href": "syntheticcontrol.html#references",
    "title": "Synthetic Control",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nAbadie, Alberto. 2021. “Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects.” Journal of Economic Literature 59 (2): 391–425. https://doi.org/10.1257/jel.20191450.\n\n\nAbadie, Alberto, Susan Athey, Guido W. Imbens, and Jeffrey M. Wooldridge. 2020. “Sampling-Based Versus Design-Based Uncertainty in Regression Analysis.” Econometrica 88 (1): 265–96. https://doi.org/https://doi.org/10.3982/ECTA12675.\n\n\nAbadie, Alberto, and Matias D. Cattaneo. 2021. “Introduction to the Special Section on Synthetic Control Methods.” Journal of the American Statistical Association 116 (536): 1713–15. https://doi.org/10.1080/01621459.2021.2002600.\n\n\nAbadie, Alberto, Alexis Diamond, and Jens Hainmueller. 2010. “Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control Program.” Journal of the American Statistical Association 105 (490): 493–505. https://doi.org/10.1198/jasa.2009.ap08746.\n\n\nCattaneo, Matias D., Yingjie Feng, Filippo Palomba, and Rocio Titiunik. 2022. “Scpi: Uncertainty Quantification for Synthetic Control Methods.”\n\n\nCattaneo, Matias D., Yingjie Feng, and Rocio Titiunik. 2021. “Prediction Intervals for Synthetic Control Methods.” Journal of the American Statistical Association 116 (536): 1865–80. https://doi.org/10.1080/01621459.2021.1979561.\n\n\nChernozhukov, Victor, Kaspar Wüthrich, and Yinchu Zhu. 2021. “An Exact and Robust Conformal Inference Method for Counterfactual and Synthetic Controls.” Journal of the American Statistical Association 116 (536): 1849–64. https://doi.org/10.1080/01621459.2021.1920957.\n\n\nFacure, Matheus. 2022. Causal Inference for the Brave and True. https://matheusfacure.github.io/python-causality-handbook/landing-page.html.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. CRC Press. https://theeffectbook.net/."
  },
  {
    "objectID": "syntheticcontrol.html#variables-to-match",
    "href": "syntheticcontrol.html#variables-to-match",
    "title": "Synthetic Control",
    "section": "Variables to Match",
    "text": "Variables to Match\n\nVector of pretreatment variables for treated unit \\[\n\\mathbf{X}_1 = \\left(Y_{11}, \\cdots Y_{1T_0}, z_{11}, \\cdots, z_1K \\right)^T\n\\]\nMatrix of same variabels for untreated \\[\n\\mathbf{X}_0 = \\begin{pmatrix}\nY_{21} & \\cdots & Y_{2T_0} & z_{21} & \\cdots & z_{2K} \\\\\n\\vdots &        &          &        &        & \\vdots \\\\\nY_{J+1,1} & \\cdots & Y_{J+1,T_0} & z_{J+1,1} & \\cdots & z_{J+1,K}\n\\end{pmatrix}^T\n\\]"
  },
  {
    "objectID": "syntheticcontrol.html#computing-weights-1",
    "href": "syntheticcontrol.html#computing-weights-1",
    "title": "Synthetic Control",
    "section": "Computing Weights",
    "text": "Computing Weights\n\nfrom scipy.optimize import fmin_slsqp\nfrom toolz import reduce, partial\n\nX1 = inverted[3].values # state of california\nX0 = inverted.drop(columns=3).values  # other states\n\ndef loss_w(W, X0, X1) -&gt; float:\n    return np.sqrt(np.mean((X1 - X0.dot(W))**2))\n\n\ndef get_w(X0, X1):\n    w_start = [1/X0.shape[1]]*X0.shape[1]\n    weights = fmin_slsqp(partial(loss_w, X0=X0, X1=X1),\n                         np.array(w_start),\n                         f_eqcons=lambda x: np.sum(x) - 1,\n                         bounds=[(0.0, 1.0)]*len(w_start),\n                         disp=False)\n    return weights"
  },
  {
    "objectID": "syntheticcontrol.html#treatment-permutation-2",
    "href": "syntheticcontrol.html#treatment-permutation-2",
    "title": "Synthetic Control",
    "section": "Treatment Permutation",
    "text": "Treatment Permutation\n\n\nCode\nplt.figure(figsize=(12,7))\nfor state in synthetic_states:\n    plt.plot(state[\"year\"], state[\"cigsale\"] - state[\"synthetic\"], color=\"C5\",alpha=0.4)\n\nplt.plot(cigar.query(\"california\")[\"year\"], cigar.query(\"california\")[\"cigsale\"] - calif_synth,\n        label=\"California\");\n\nplt.vlines(x=1988, ymin=-50, ymax=120, linestyle=\":\", lw=2, label=\"Proposition 99\")\nplt.hlines(y=0, xmin=1970, xmax=2000, lw=3)\nplt.ylabel(\"Gap in per-capita cigarette sales (in packs)\")\nplt.title(\"State - Synthetic Across Time\")\nplt.legend()\nplt.show()"
  }
]